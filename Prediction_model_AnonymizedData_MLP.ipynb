{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping rows with NaN in 'reach': 22634\n",
      "Column names in the dataset:\n",
      "['engagement', 'ad_name', 'campaign', 'date', 'reach', 'spend']\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22634 entries, 0 to 22753\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   engagement  22634 non-null  int64         \n",
      " 1   ad_name     22634 non-null  object        \n",
      " 2   campaign    22634 non-null  object        \n",
      " 3   date        22634 non-null  datetime64[ns]\n",
      " 4   reach       22634 non-null  int64         \n",
      " 5   spend       22634 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "   engagement                                            ad_name  \\\n",
      "0        3771  XBrand | Save More | Zero Interest | Easy Paym...   \n",
      "1       12798  XBrand | Weekend Treat | Easy Payments | Trave...   \n",
      "2          54  XBrand | Gift Vouchers | Limited Time | Win Bi...   \n",
      "3         556  XBrand | Win Big | Swipe and Win | Easy Paymen...   \n",
      "4          61  XBrand | Get Rewarded | Win Big | Zero Interes...   \n",
      "\n",
      "                                            campaign       date  reach  \\\n",
      "0  XBrand | Green Finance | Holiday Cashback | Sa... 2023-05-01  44392   \n",
      "1  XBrand | Luxury Travel | Holiday Cashback | Li... 2023-05-01  26094   \n",
      "2  XBrand | Savings Fiesta | Luxury Travel | Diwa... 2023-05-01  52160   \n",
      "3  XBrand | Lifestyle Rewards | Savings Fiesta | ... 2023-05-01   8097   \n",
      "4  XBrand | Flash Sale | Gold Loan Offers | Back ... 2023-05-01  41825   \n",
      "\n",
      "     spend  \n",
      "0   748.92  \n",
      "1   702.28  \n",
      "2   974.39  \n",
      "3   947.15  \n",
      "4  1018.62  \n",
      "\n",
      "Summary statistics:\n",
      "          engagement                           date          reach  \\\n",
      "count   22634.000000                          22634   22634.000000   \n",
      "mean     1978.862861  2024-07-29 06:43:17.649553664   22255.566095   \n",
      "min         0.000000            2023-05-01 00:00:00       0.000000   \n",
      "25%        21.000000            2024-04-02 00:00:00    2832.250000   \n",
      "50%        96.000000            2024-09-01 00:00:00   10930.500000   \n",
      "75%       829.750000            2024-12-27 00:00:00   24923.250000   \n",
      "max    321262.000000            2025-04-17 00:00:00  917644.000000   \n",
      "std      8792.670109                            NaN   41480.994541   \n",
      "\n",
      "               spend  \n",
      "count   22634.000000  \n",
      "mean     1643.144565  \n",
      "min         0.030000  \n",
      "25%       391.590000  \n",
      "50%       771.105000  \n",
      "75%      1615.085000  \n",
      "max    107843.880000  \n",
      "std      3353.606923  \n",
      "\n",
      "Missing values:\n",
      "spend         0\n",
      "campaign      0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "\n",
      "Negative values:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the dataset from Excel\n",
    "df = pd.read_excel('Data/XBrand_FB_Main_metrics_Anonymized_dataset.xlsx')\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Clean and convert 'reach' column to numeric (remove commas and handle invalid entries)\n",
    "# Replace invalid entries like ' -   ' with NaN\n",
    "df['reach'] = df['reach'].astype(str).str.replace(',', '').str.strip()  # Remove commas and strip whitespace\n",
    "df['reach'] = pd.to_numeric(df['reach'], errors='coerce')  # Convert to numeric, invalid entries become NaN\n",
    "\n",
    "# Optionally, drop rows where 'reach' is NaN (since 'reach' is a target variable)\n",
    "df = df.dropna(subset=['reach'])\n",
    "print(f\"Number of rows after dropping rows with NaN in 'reach': {len(df)}\")\n",
    "\n",
    "# Convert 'reach' to integer now that invalid entries are handled\n",
    "df['reach'] = df['reach'].astype(int)\n",
    "\n",
    "# Printing the exact column names to identify any discrepancies\n",
    "print(\"Column names in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Displaying basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df[['spend', 'campaign', 'reach', 'engagement']].isna().sum())\n",
    "\n",
    "# Checking for negative values (since these metrics should be non-negative)\n",
    "print(\"\\nNegative values:\")\n",
    "print((df[['spend', 'reach', 'engagement']] < 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in raw df:\n",
      "spend           0\n",
      "reach           1\n",
      "engagement    906\n",
      "dtype: int64\n",
      "Number of rows after dropping NaN: 21848\n",
      "NaN in df after log transformation:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Inf in df after log transformation:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Spend log-transformed quantiles (before scaling): 1st percentile: 2.5705, 99th percentile: 9.5649\n",
      "NaN in df after clipping:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Inf in df after clipping:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Scaler mean: [9.0470371  5.01718797]\n",
      "Scaler scale: [1.64824353 2.31993538]\n",
      "NaN in df after scaling:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Inf in df after scaling:\n",
      "spend         0\n",
      "reach         0\n",
      "engagement    0\n",
      "dtype: int64\n",
      "Number of rows after dropping NaN/inf: 21848\n",
      "\n",
      "Unique campaigns:\n",
      "['HNB | Credit Cards | April Offers 2023 | Auction Reach | RO 64550 | March 2023'\n",
      " 'HNB | Credit Cards | April Offers 2023 | Auction Engagement | RO 64550 | April 2023'\n",
      " 'HNB | May Monthly Boosting | Auction Reach | RO 63711 | May 2023'\n",
      " 'HNB | May Monthly Boosting | Auction Engagement | RO 63711 | May 2023'\n",
      " 'HNB | April Monthly Boosting | Auction Reach | RO 63710 | April 2023'\n",
      " 'HNB | April Monthly Boosting | Auction Engagement | RO 63710 | April 2023'\n",
      " 'HNB | Savings Campaign | Facebook Video | Auction Reach | RO 64432 | March 2023'\n",
      " 'HNB | Savings Campaign | Facebook Display | Auction Reach | RO 64433 | March 2023'\n",
      " 'HNB | Savings Campaign | Facebook Display | Auction Engagement | RO 64433 | March 2023'\n",
      " 'HNB | June Monthly Boosting | Auction Reach | RO 63712 | June 2023'\n",
      " 'HNB | June Monthly Boosting | Auction Engagement | RO 63712 | June 2023'\n",
      " 'HNB | Leasing | Flexi plans | Lead Gen | RO 65591 | June 2023'\n",
      " 'HNB | July Monthly Boosting | Auction Reach | RO 66292 | July 2023'\n",
      " 'HNB | July Monthly Boosting | Auction Engagement | RO 66292 | July 2023'\n",
      " 'HNB | Digital Banking | Phase 3 | Auction Reach | RO 66245 | July 2023'\n",
      " 'HNB | Digital Banking | Phase 3 | Auction Engagement | RO 66245 | July 2023'\n",
      " 'HNB | Salary Smart | Facebook - July | Lead Gen | RO 66466 | July 2023'\n",
      " 'HNB | Credit Cards | Lead campaign - July | Lead Gen | RO 66521 | July 2023'\n",
      " 'HNB | Diaspora campaign | July | Auction Reach | RO 66561 | July 2023'\n",
      " 'HNB | Diaspora campaign | July | Auction Engagement | RO 66561 | July 2023'\n",
      " 'HNB | August Monthly Boosting | Auction Reach | RO 66293 | July 2023'\n",
      " 'HNB | August Monthly Boosting | Auction Engagement | RO 66293 | July 2023'\n",
      " 'HNB | Diaspora campaign | July | Auction Traffic | RO 66561 | August 2023'\n",
      " 'HNB | Youth Campaign | Facebook | Auction Reach | RO 66773 | August 2023'\n",
      " 'HNB | Youth Campaign | Facebook | Auction Engagement | RO 66773 | August 2023'\n",
      " 'HNB | FC Advantage | Leads campaign | Lead Gen | RO 66909 | August 2023'\n",
      " 'HNB | September Monthly Boosting | Auction Reach | RO 66293 | September 2023'\n",
      " 'HNB | September Monthly Boosting | Auction Engagement | RO 66293 | September 2023'\n",
      " 'HNB | October Monthly Boosting | Auction Reach | RO 66295 | October 2023'\n",
      " 'HNB | October Monthly Boosting | Auction Engagement | RO 66295 | October 2023'\n",
      " 'HNB | October Monthly Boosting - 2023 | Auction Conversion | RO 66295 | October 2023'\n",
      " 'HNB | October Monthly Boosting | Auction Traffic | RO 66295 | October 2023'\n",
      " 'HNB | Giftober 2023 | Facebook Reach | Auction Reach | RO 67366 | October 2023'\n",
      " 'HNB | Giftober 2023 | Facebook Engagement | Auction Engagement | RO 66368 | October 2023'\n",
      " 'HNB | Giftober 2023 | Facebook Reach | Auction Engagement | RO 67366 | October 2023'\n",
      " 'HNB | November Monthly Boosting | Auction Reach | RO 66296 | November 2023'\n",
      " 'HNB | November Monthly Boosting | Auction Engagement | RO 66296 | November 2023'\n",
      " 'HNB | Seasonal Campaign Advertising Budget November/December 2023 | Auction Reach | RO 68076 | November 2023'\n",
      " 'HNB | November Monthly Boosting | Auction Traffic | RO 66296 | November 2023'\n",
      " 'HNB | Seasonal Campaign Advertising Budget November/December 2023 | Auction Engagement | RO 68076 | November 2023'\n",
      " 'HNB | Seasonal Fashion Offer Carousel FB | Auction Traffic | RO 68076 | December 2023'\n",
      " 'HNB | December Monthly Boosting | Auction Reach | RO 66720 | December 2023'\n",
      " 'HNB | December Monthly Boosting | Auction Engagement | RO 66720 | December 2023'\n",
      " 'HNB | December Monthly Boosting | Lead Gen | RO 66720 | December 2023'\n",
      " 'HNB | December Monthly Boosting - 2023 | Auction Traffic | RO 66720 | December 2023'\n",
      " 'HNB | Seasonal Campaign Meta Advertising Budget November / December 2023 | Auction Reach | RO 68076 | December 2023'\n",
      " 'HNB | December Monthly Boosting | Video Views | RO 66720'\n",
      " 'HNB | Seasonal Campaign Meta Advertising Budget November/December 2023 | Auction Traffic | RO 68076 | December 2023'\n",
      " 'HNB | Seasonal Campaign Meta Advertising Budget November/December 2023 | Auction Views | RO 68076 | January 2024'\n",
      " 'HNB | Meta January Monthly Boosting - 2024 | Auction Reach | RO 68674 | January 2024'\n",
      " 'HNB | Meta January Monthly Boosting - 2024 | Auction Engagement | RO 68674 | January 2024'\n",
      " 'HNB | Meta January Monthly Boosting - 2024 | Auction Traffic | RO 68674 | January 2024'\n",
      " 'HNB | Meta February Monthly Boosting | Auction Reach | RO 68927 | February 2024'\n",
      " 'HNB | Meta February Monthly Boosting | Auction Engagement | RO 68927 | February 2024'\n",
      " 'HNB | Technnovation February 2024 | Auction Traffic | RO 69023 | February 2024'\n",
      " 'HNB | Meta February Monthly Boosting | Auction Traffic | RO 68927 | February 2024'\n",
      " 'HNB | Ithi Pahan February 2024 | Auction Engagement | RO 69105 | February 2024'\n",
      " 'HNB | Ithi Pahan February 2024 | Auction Views | RO 69105 | February 2024'\n",
      " 'HNB | Meta February Monthly Boosting | Lead Gen | RO 68927 | February 2024'\n",
      " 'HNB | Meta March Monthly Boosting | Auction Traffic | RO 69317 | March 2024'\n",
      " 'HNB | Meta March Monthly Boosting | Auction Reach | RO 69317 | March 2024'\n",
      " 'HNB | Meta March Monthly Boosting | Lead Gen | RO 69317 | March 2024'\n",
      " 'HNB | Meta March Monthly Boosting | Auction Views | RO 69317 | March 2024'\n",
      " 'HNB | Stamp Launch Video | Auction Views | RO 69508 | March 2024'\n",
      " 'HNB | Stamp Launch Video | Auction Reach | RO 69508 | March 2024'\n",
      " 'HNB | Meta March Monthly Boosting | Auction Engagement | RO 69317 | March 2024'\n",
      " 'HNB | Avrudu Meta Advertising 2024 Flipbook clicks | Auction Traffic | RO 69688 | March 2024'\n",
      " 'HNB | Avrudu Meta Content Advertising | Auction Reach | RO 69687 | March 2024'\n",
      " 'HNB | Avrudu Meta Content Advertising | Auction Engagement | RO 69687 | March 2024'\n",
      " 'HNB | Singhithi Jumbo April 2024 Meta Advertising | Auction Reach | RO 69778 | April 2024'\n",
      " 'HNB | Savings Composite Campaign 2024 - Meta Advertising 2024 | Auction Reach | RO 69830 | April 2024'\n",
      " 'HNB | Meta April Monthly Boosting | Auction Reach | RO 69844 | April 2024'\n",
      " 'HNB | Gami Pubudu | Auction Reach | RO 69873 | April 2024'\n",
      " 'HNB | Avurudu Meta Advertising | Ask Habio Clicks 2024 | Auction Traffic | RO 69689 | April 2024'\n",
      " 'HNB | Meta April Monthly Boosting | Auction Engagement | RO 69844 | April 2024'\n",
      " 'HNB | Avrudu Meta Content Advertising 2024 | Auction Traffic | RO 69687 | April 2024'\n",
      " 'HNB | April 2024 Meta Advertising | Auction Traffic | RO 69844 | April 2024'\n",
      " 'HNB | Avrudu Meta Advertising 2024 Flipbook clicks | Auction Reach | RO 69688 | April 2024'\n",
      " 'HNB | Community Pages | Avurudu 2024 | Auction Reach | RO 69984 | April 2024'\n",
      " 'HNB | Avrudu Meta Content Advertising 2024 | Auction Views | RO 69687 | April 2024'\n",
      " 'HNB | Savings Composite Campaign 2024 - Meta Advertising 2024 Videos | Auction Reach | RO 69830 | May 2024'\n",
      " 'HNB | May Monthly Boosting | Auction Reach | RO 70236 | May 2024'\n",
      " 'HNB | May Monthly Boosting | Auction Engagement | RO 70236 | May 2024'\n",
      " 'HNB | Mothers Day | Auction Engagement | RO 70237 | May 2024'\n",
      " 'HNB | May 2024 Live Boosting - Awareness RO 70236'\n",
      " 'HNB | Savings Composite Campaign 2024 - Meta Advertising 2024 Static | Auction Reach | RO 69830 | May 2024'\n",
      " 'HNB | June Monthly Boosting | Auction Reach RO 70350'\n",
      " 'HNB | June Monthly Boosting | Auction Views | RO 70350 | May 2024'\n",
      " 'HNB | Cric Fever boosting | Auction Engagement | RO 70477 | June 2024'\n",
      " 'HNB | June Monthly boosting | Auction Reach | RO 70350 | June 2024'\n",
      " 'HNB | June Monthly Boosting | Andriod Auction App Promotion | - RO 70350'\n",
      " 'HNB | June Monthly Boosting | Auction App Promotion | - RO 70350'\n",
      " 'HNB | Cric Fever boosting | Auction Reach | RO 70477 | June 2024'\n",
      " 'HNB | July Monthly Boosting | Auction Views | RO 70648 | July 2024'\n",
      " 'HNB | July Monthly Boosting | Auction Engagement | RO 70648 | July 2024'\n",
      " 'HNB | July Monthly Boosting | App Promotion | RO 70648 | July 2024'\n",
      " 'HNB | July Monthly boosting | Auction Reach | RO 70648 | July 2024'\n",
      " 'HNB | July Monthly Boosting | Auction Leads | RO 70648 | July 2024'\n",
      " 'HNB | July 2024 Monthly boosting | App promotion ios  | RO 70648 | July 2024'\n",
      " 'HNB App Downloads Meta July 2024 | Auction reach | RO 70803 | July 2024'\n",
      " 'HNB App Downloads Meta July 2024 | Auction App promotion |RO 70803| Rs7,500'\n",
      " 'HNB - DKYC Meta Advertising July / August 2024 | Auction Reach | RO 70912 | July 2024'\n",
      " 'HNB Adiveil Meta Ads July 2024 | Auction Views | RO 70910 | July 2024'\n",
      " 'HNB Adiveil Meta Ads July 2024 | Auction Views (reach ) | RO 70910 | July 2024'\n",
      " 'HNB | FC Leads Campaign | Auction Leads | RO 70911'\n",
      " 'HNB Meta August 2024 Advertising | Auction Engagement| RO 70989 | August 2024'\n",
      " 'HNB Meta August 2024 Advertising | Auction Reach | RO 70989 | August 2024'\n",
      " 'HNB | FC Leads Campaign | Lead Gen | RO 70911 | July 2024'\n",
      " 'HNB Meta August 2024 Advertising |App promtion | RO 70989 | August 2024'\n",
      " 'HNB | Esala Perahara Boosting 2024 | RO 71152 | Auction video views | August 2024'\n",
      " 'HNB | Esala Perahara Boosting 2024 | RO 71152 | Auction Engagement | August 2024'\n",
      " 'HNB Meta August 2024 Advertising | Auction Engagement | RO 70989 | August 2024'\n",
      " 'HNB Easy Draft Leads Meta 2024 | Leads | RO 71159 | August 2024'\n",
      " 'HNB Meta August 2024 Advertising | Auction Views | RO 70989 | August 2024'\n",
      " 'HNB September Meta Advertising  | Auction Reach | RO 71193 | September 2024'\n",
      " 'HNB | September Meta Advertising | Auction Engagement | RO 71193 | September 2024'\n",
      " 'HNB | October Meta Advertising 2024 | Auction Views | RO 71505 | September 2024'\n",
      " 'HNB | Digital Banking Campaign Meta | Auction Views | RO 71473 | September 2024'\n",
      " 'HNB | Digital Banking Campaign | Digital Banking Products | Digital Banking App | Campaign | Engagement | 71473 | 20-Sep-2024 | 24-Oct-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | Credit Cards | Offers | Monthly Boosting  | Reach | 71505 | 20-Sep-2024 | 27-Oct-2024'\n",
      " 'HNB | HNB Remittance Campaign | Remittance | Foreign Remittances in general | Campaign | Leads | 71805 | 10-Oct-2024 | 30-Nov-2024'\n",
      " 'HNB | HNB Remittance Campaign | Remittance | Foreign Remittances in general | Campaign | Reach | 71805 | 14-Oct-2024 | 30-Nov-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | Credit Cards | Offers | Monthly Boosting  | Engagement | 71505 | 20-Sep-2024 | 27-Oct-2024'\n",
      " 'HNB | HNB Sighithi Giftober October 2024 | Deposit Mobilization Products | Singithi | Campaign | Reach | 71847 | 14-Oct-2024 | 18-Nov-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | Credit Cards | Offers | Monthly Boosting  | Video Views | 71505 | 20-Sep-2024 | 27-Oct-2024'\n",
      " 'HNB | HNB PFS Campaign October 2024 | Personal Financial Services | Home Loans | Campaign | Leads | 71844 | 10-Oct-2024 | 11-Nov-2024'\n",
      " 'HNB | HNB PFS Campaign October 2024 | Personal Financial Services | Personal Loans | Campaign | Leads | 71844 | 10-Oct-2024 | 11-Nov-2024'\n",
      " 'HNB | HNB PFS Campaign October 2024 | Personal Financial Services | Education Loans | Campaign | Leads | 71844 | 10-Oct-2024 | 11-Nov-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | Leasing | Vehicle Leasing | Monthly Boosting | Reach | 71505 | 23-Oct-2024 | 31-Oct-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | Personal Financial Services | PFS | Monthly Boosting  | Reach | 71505 | 23-Oct-2024 | 31-Oct-2024'\n",
      " 'HNB | HNB Sighithi Giftober October 2024 | Deposit Mobilization Products | Singithi | Campaign | Engagement | 71847 | 28-Oct-2024 | 18-Nov-2024'\n",
      " 'HNB | October 2024 Meta Monthly Boosting | General | General Banking | Monthly Boosting  | Reach | 71505 | 31-Oct-2024 | 07-Nov-2024'\n",
      " 'HNB | November Monthly Boosting 2024 | Credit Cards | Offers | Monthly Boosting  | Reach | 72128 | 05-Nov-2024 | 30-Nov-2024'\n",
      " 'HNB | November Monthly Boosting 2024 | Corporate | General Corporate | Monthly Boosting  | Reach | 72128 | 06-Nov-2024 | 30-Nov-2024'\n",
      " 'HNB | November Monthly Boosting 2024 | Leasing | Vehicle Leasing | Monthly Boosting  | Reach | 72128 | 06-Nov-2024 | 30-Nov-2024'\n",
      " 'HNB | November Monthly Boosting 2024 | Personal Financial Services | Personal Loans | Monthly Boosting  | Reach | 72128 | 06-Nov-2024 | 30-Nov-2024'\n",
      " 'HNB | HNB Seasonal Campaign 2024 Meta Boosting | Credit Cards | Offers | Seasonal Campaign | Reach | 72289 | 18-Nov-2024 | 31-Dec-2024'\n",
      " 'HNB | HNB Seasonal Campaign 2024 Meta Boosting | Credit Cards | Offers | Seasonal Campaign | Link Clicks | 72289 | 18-Nov-2024 | 31-Dec-2024'\n",
      " 'HNB | HNB Cards Cruise Campaign 2024 Meta Boosting | Credit Cards | Offers | Cards Cruise Campaign | Reach | 72316 | 18-Nov-2024 | 31-Dec-2024'\n",
      " 'HNB | HNB Singithi Day Reach Video Views November 2024 | Singhithi  | Offers | Singithi Day Video | Reach | 72320 | 18-Nov-2024 | 30-Nov-2024'\n",
      " 'HNB Seasonal Carousel Offers - Templates 72289'\n",
      " 'HNB | November Monthly Boosting 2024 | Credit Cards | Offers | Monthly Boosting  | Link Clicks | 72128 | 29-Nov-2024 | 10-Dec-2024'\n",
      " 'HNB | HNB Teen+ Netball Advertising  | General | General Banking | Teen+ Netball Static 1 | Reach | 72418 | 02-Dec-2024 | 09-Dec-2024'\n",
      " 'HNB | December Monthly Boosting 2024 | Credit Cards | Offers | Monthly Boosting  | Reach | 72449 | 04-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | December Monthly Boosting 2024 | Leasing | Vehicle Leasing | Monthly Boosting  | Reach | 72449 | 05-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | December Monthly Boosting 2024 | General | General Banking | Monthly Boosting  | Reach | 72449 | 05-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | December Monthly Boosting 2024 | General | General Banking | Monthly Boosting  | Engagement | 72449 | 05-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | Santa Brigade Meta Advertising 2024 | General | General Banking | Campaign | Traffic (Clicks) | 72772 | 18-Dec-2024 | 30-Dec-2024'\n",
      " 'HNB | Santa Brigade Meta Advertising 2024 | General | General Banking | Campaign | Reach (Views) | 72772 | 18-Dec-2024 | 30-Dec-2024'\n",
      " 'HNB | December Additional Meta Advertising 2024 | General | General Banking | Campaign | Reach | 72812 | 23-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | December Additional Meta Advertising 2024 | Credit Cards | Offers | Campaign | Reach | 72812 | 23-Dec-2024 | 31-Dec-2024'\n",
      " 'HNB | Hatnafluencer Campaign Meta Advertising January 2025 | General | General Banking | Campaign | Thruplays | 72976 | 10-Jan-2025 | 31-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2024 | General | General Banking | Monthly Boosting  | Reach | 72975 | 13-Jan-2025 | 19-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | General | General Banking | Monthly Boosting  | Engagement | 72975 | 13-Jan-2025 | 19-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | General | General Banking | Monthly Boosting  | Thruplays | 72975 | 13-Jan-2025 | 19-Jan-2025'\n",
      " 'HNB | HNB Anniversary Testimonials January 2025 | General | General Banking | Campaign | VideoViews | 72974 | 10-Jan-2025 | 10-Feb-2025'\n",
      " 'HNB | January Monthly Boosting 2024 | General | General Banking | Monthly Boosting | Reach | 72975 | 20-Jan-2025 | 31-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | Credit Cards | Offers | Monthly Boosting | Reach | 72975 | 13-Jan-2025 | 31-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | General | General Banking | Monthly Boosting | Link Clicks | 72975 | 13-Jan-2025 | 19-Jan-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | Digital Banking Products | Digital Banking App | Campaign | Reach | 72975 | 31-Jan-2025 | 07-Feb-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | Corporate | General Banking | Campaign | Reach | 72975 | 31-Jan-2025 | 07-Feb-2025'\n",
      " 'HNB | January Monthly Boosting 2025 | Leasing | Leasing | Campaign | Reach | 72975 | 31-Jan-2025 | 07-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | General | General Banking | Independence Day | Engagement | 73282 | 03-Feb-2025 | 10-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | Credit Cards | Offers | Monthly Boosting  | Reach | 73282 | 06-Feb-2025 | 28-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | General | General Banking | Digital Banking February 2025 | Reach | 73282 | 07-Feb-2025 | 28-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | Leasing | Vehicle Leasing | BYD  | Reach | 73282 | 10-Feb-2025 | 28-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | Digital Banking Products | Digital Banking App | Monthly Boosting  | Link Clicks | 73282 | 10-Feb-2025 | 16-Feb-2025'\n",
      " 'HNB | February 2025 Technnovation_Vote | Digital Banking Products | Digital Banking App | Facebook | Link Clicks | 73408 | 10-Feb-2025 | 14-Feb-2025'\n",
      " 'HNB | Singapore Tour Campaign 2025 | Credit Cards | Offers | Campaign | Reach | 73397 | 10-Feb-2025 | 31-Mar-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | General | Investment Plans | Monthly Boosting | Reach | 73282 | 11-Feb-2025 | 18-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | General | General Banking | Monthly Boosting  | Reach | 73282 | 11-Feb-2025 | 28-Feb-2025'\n",
      " 'HNB | February Monthly Boosting 2025 | Loans | Home Loans | Home Loans  | Reach | 73282 | 13-Feb-2025 | 28-Feb-2025'\n",
      " 'HNB | Valentine Day Engagment Campaign | General | General Corporate | Campaign | Reach | 73427 | 14-Feb-2025 | 15-Feb-2025'\n",
      " 'HNB | Valentine Day Engagment Campaign | General | General Corporate | Campaign | Engagement | 73427 | 14-Feb-2025 | 15-Feb-2025'\n",
      " 'HNB | Singapore Tour Campaign 2025 | Credit Cards | Offers | Campaign | Reach | 73397 | 11-Feb-2025 | 31-Mar-2025'\n",
      " 'HNB | MakingYouHappy | Corporate | General Corporate | Thematic | R&F: Thruplays | 73470 | 17-Feb-2025 | 03-Mar-2025'\n",
      " 'HNB | Singapore Tour Campaign 2025 | Credit Cards | Offers | Campaign | Reach | 73397 | 11-Feb-2025 | 31-Mar-2025 â€“ Copy'\n",
      " 'HNB | Corporate Campaign_Tactical Drive  | Personal Finance Services | Home Loans | Tactical Drive | R & F | 73712 | 04-Mar-2023 | 04-Apr-2023'\n",
      " 'HNB | Corporate Campaign_Tactical Drive | Personal Finance Services | Home Loans | Tactical Drive | Leads | 73713 | 04-Mar-2023 | 04-Apr-2023'\n",
      " \"HNB | HNB Women's Day Campaign | Corporate | General Corporate | Campaign | Reach | 73797 | 08-Mar-2025 | 10-Mar-2025\"\n",
      " \"HNB | HNB Women's Day Campaign | Corporate | General Corporate | Campaign | Engagement | 73798 | 08-Mar-2025 | 10-Mar-2025\"\n",
      " 'HNB | March Monthly Boosting 2025 - Reach  | General | General Corporate | Campaign | Reach | 73795 | 08-Mar-2025 | 12-Mar-2025'\n",
      " 'HNB | March Monthly Boosting 2025 - Engagment | General | General Corporate | Campaign | Engagement | 73795 | 08-Mar-2025 | 12-Mar-2025'\n",
      " 'HNB | HNB Tactical Drive - Salary Smart 2025 | Deposit Mobilization Products | Salary Smart | Campaign | R & F | 73707 | 12-Mar-2025 | 10-Apr-2025'\n",
      " 'HNB | HNB Tactical Drive - Investment 2025 | Deposit Mobilization Products | Investment Plans | Campaign | R & F | 73709 | 12-Mar-2025 | 10-Apr-2025'\n",
      " 'HNB | HNB Tactical Drive - Investment 2025 | Deposit Mobilization Products | Investment Plans | Campaign | Link Clicks | 73710 | 12-Mar-2025 | 10-Apr-2025'\n",
      " 'HNB | HNB Tactical Drive - Salary Smart 2025 | Deposit Mobilization Products | Salary Smart | Campaign | Link Clicks | 73708 | 12-Mar-2025 | 10-Apr-2025'\n",
      " 'HNB | March Monthly Boosting 2025 - Reach  | General | General Corporate | Campaign | Reach | 73795 | 14-Mar-2025 | 31-Mar-2025'\n",
      " \"HNB | HNB Awurudu Campaign'25  | Credit Cards | Offers | Campaign | Reach | 73899 | 19-Mar-2025 | 30-Apr-2025\"\n",
      " 'HNB | MakingYouHappy | Corporate | General Corporate | Thematic | Engagement | 73480 | 15-Mar-2025 | 31-Mar-2025'\n",
      " 'HNB | Remittance Campaign  | Remitance | Foreign Remittances in general | Campaign | Engagement | 73980 | 25-Mar-2025 | 26-Mar-2025'\n",
      " 'HNB | Foreign Currency Investment  | Deposit Mobilization Products | Investment Plans | Campaign | Leads | 73976 | 25-Mar-2025 | 24-Apr-2025'\n",
      " 'HNB | MakingYouHappy | Corporate | General Corporate | Thematic | R&F: Reservation | 73795 | 27-Mar-2025 | 05-Apr-2025'\n",
      " 'HNB | Remittance Campaign Launch Sender Sinhala | Remitance | Foreign Remittances in general | Campaign | Engagement - ThruPlays | 73980 | 26-Mar-2025 | 10-Apr-2025'\n",
      " 'online fashion - Set 1'\n",
      " 'HNB | Remittance Campaign Launch Sender S & T | Remitance | Foreign Remittances in general | Campaign | Thruplays | 73980 | 28-Mar-2025 | 27-Apr-2025 | asia'\n",
      " \"HNB | Gami Pubuduwa Campaign'25 | Corporate | General Banking | Campaign | Reach |  | 29-Mar-2025 | 06-Apr-2025\"\n",
      " 'HNB | Remittance Campaign Launch Reciever S&T | Remitance | Foreign Remittances in general | Campaign | R&F: Thruplays | 73980 | 26-Mar-2025 | 27-Apr-2025'\n",
      " 'HNB | Remittance Campaign Launch Sender S&T | Remitance | Foreign Remittances in general | Campaign | Engagement | 73980 | 02-Apr-2025 | 27-Apr-2025 | Sinhala'\n",
      " 'HNB | SOLO Merchants Campaign | Digital Banking Products | SOLO | Campaign | Traffic (Clicks) | 74126 | 03-Apr-2025 | 04-May-2025'\n",
      " 'HNB | SOLO Merchants Campaign | Digital Banking Products | SOLO | Campaign | R & F | 74125 | 03-Apr-2025 | 04-May-2025'\n",
      " 'HNB | Reserved For You | Tactical Campaign | Leasing | Vehicle Leasing |  | R & F | 74163 | 04-Apr-2025 | 30-Apr-2025'\n",
      " 'HNB | Remittance Campaign Launch Reciever S&T | Remitance | Foreign Remittances in general | Campaign | Engagement | 73980 | 02-Apr-2025 | 27-Apr-2025'\n",
      " \"HNB | Gami Pubuduwa Campaign'25 | Corporate | General Banking | Campaign | Reach | 73987 | 29-Mar-2025 | 06-Apr-2025\"\n",
      " 'HNB | Singithi Jumbo Avurudu Campaign 2025 | Deposit Mobilization Products | Singithi | Campaign | Reach | 74136 | 03-Apr-2025 | 21-Apr-2025'\n",
      " 'HNB | Reserved For You | Tactical Campaign | Leasing | Vehicle Leasing |  | Clicks | 74163 | 04-Apr-2025 | 30-Apr-2025'\n",
      " 'HNB | April Monthly Boosting - 2025 | General | General Corporate | Monthly Boosting | Reach | 74161 | 07-Apr-2025 | 30-Apr-2025'\n",
      " 'HNB | April Monthly Boosting - 2025 | General | General Corporate | Monthly Boosting | Reach | 74161 | 10-Apr-2025 | 30-Apr-2025'\n",
      " 'HNB | Remittance Campaign Launch Sender S & T | Remitance | Foreign Remittances in general | Campaign | Auction: Thruplays | 73980 | 28-Mar-2025 | 27-Apr-2025 | Sinhala Creatives'\n",
      " 'HNB | Remittance Campaign Launch Sender S & T | Remitance | Foreign Remittances in general | Campaign | Auction: Thruplays | 73980 | 28-Mar-2025 | 27-Apr-2025 | Tamil'\n",
      " 'HNB | Avurudu Main Campaign  | Corporate | General Corporate | Campaign  | R & F | 74254 | 11-Apr-2025 | 16-Apr-2025'\n",
      " \"HNB | Gami Pubuduwa Campaign'25 | Corporate | General Banking | Campaign | Engagement | 73987 | 29-Mar-2025 | 06-Apr-2025\"\n",
      " 'HNB | Avurudu Main Campaign  | Corporate | General Corporate | Campaign  | Engagement | 74254 | 11-Apr-2025 | 20-Apr-2025'\n",
      " 'HNB | Avurudu Main Campaign - April Monthly Boosting | Corporate | General Corporate | Campaign  | Reach | 74161 | 11-Apr-2025 | 20-Apr-2025'\n",
      " 'HNB | April Monthly Boosting - 2025 | General | General Corporate | Monthly Boosting | Engagement | 74161 | 07-Apr-2025 | 30-Apr-2025']\n",
      "Feature stats after scaling:\n",
      "               spend\n",
      "count  2.184800e+04\n",
      "mean  -4.344952e-16\n",
      "std    1.000023e+00\n",
      "min   -3.611568e+00\n",
      "25%   -6.337061e-01\n",
      "50%   -4.007962e-02\n",
      "75%    6.119277e-01\n",
      "max    2.465716e+00\n",
      "Target stats after scaling:\n",
      "               reach    engagement\n",
      "count  2.184800e+04  2.184800e+04\n",
      "mean  -5.645836e-16 -1.248848e-16\n",
      "std    1.000023e+00  1.000023e+00\n",
      "min   -3.323810e+00 -1.863863e+00\n",
      "25%   -5.540178e-01 -7.751561e-01\n",
      "50%    1.882085e-01 -1.484348e-01\n",
      "75%    6.729891e-01  7.808511e-01\n",
      "max    1.924203e+00  2.277532e+00\n",
      "\n",
      "Target distribution in training set:\n",
      "              reach    engagement\n",
      "count  17478.000000  17478.000000\n",
      "mean      -0.003573     -0.004464\n",
      "std        1.003916      1.001664\n",
      "min       -3.323810     -1.863863\n",
      "25%       -0.556247     -0.775156\n",
      "50%        0.184583     -0.152482\n",
      "75%        0.669889      0.774625\n",
      "max        1.924203      2.277532\n",
      "\n",
      "Target distribution in validation set:\n",
      "             reach   engagement\n",
      "count  2185.000000  2185.000000\n",
      "mean      0.037361     0.019718\n",
      "std       0.967101     0.995044\n",
      "min      -3.323810    -1.863863\n",
      "25%      -0.522453    -0.758250\n",
      "50%       0.227283    -0.113623\n",
      "75%       0.693842     0.811894\n",
      "max       1.924203     2.277532\n",
      "\n",
      "Target distribution in test set:\n",
      "             reach   engagement\n",
      "count  2185.000000  2185.000000\n",
      "mean     -0.008782     0.015993\n",
      "std       1.000861     0.991869\n",
      "min      -3.323810    -1.863863\n",
      "25%      -0.580955    -0.758250\n",
      "50%       0.181122    -0.117355\n",
      "75%       0.671538     0.787792\n",
      "max       1.924203     2.277532\n",
      "\n",
      "Training samples: 17478, Validation samples: 2185, Test samples: 2185\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # defining features and targets\n",
    "# numerical_features = ['spend']\n",
    "# categorical_features = ['campaign']\n",
    "# targets = ['reach', 'engagement']\n",
    "\n",
    "# # debug: checking for NaN in raw data\n",
    "# print(\"NaN in raw df:\")\n",
    "# print(df[numerical_features + targets].isna().sum())\n",
    "\n",
    "# # dropping rows with NaN values in numerical features or targets\n",
    "# df = df.dropna(subset=numerical_features + targets)\n",
    "# print(f\"Number of rows after dropping NaN: {len(df)}\")\n",
    "\n",
    "# # applying log transformation to handle skewness (np.log1p handles zeros)\n",
    "# df['spend'] = np.log1p(df['spend'])\n",
    "# df['reach'] = np.log1p(df['reach'])\n",
    "# df['engagement'] = np.log1p(df['engagement'])\n",
    "\n",
    "# # debug: checking for NaN and inf values after log transformation\n",
    "# print(\"NaN in df after log transformation:\")\n",
    "# print(df[numerical_features + targets].isna().sum())\n",
    "# print(\"Inf in df after log transformation:\")\n",
    "# print(np.isinf(df[numerical_features + targets]).sum())\n",
    "\n",
    "# # storing quantiles of spend in log-transformed space (before scaling)\n",
    "# spend_log_lower = df['spend'].quantile(0.01)\n",
    "# spend_log_upper = df['spend'].quantile(0.99)\n",
    "# print(f\"Spend log-transformed quantiles (before scaling): 1st percentile: {spend_log_lower:.4f}, 99th percentile: {spend_log_upper:.4f}\")\n",
    "\n",
    "# # clipping outliers at the 99th percentile\n",
    "# df['spend'] = df['spend'].clip(lower=0, upper=df['spend'].quantile(0.99))\n",
    "# df['reach'] = df['reach'].clip(lower=0, upper=df['reach'].quantile(0.99))\n",
    "# df['engagement'] = df['engagement'].clip(lower=0, upper=df['engagement'].quantile(0.99))\n",
    "\n",
    "# # additionally clipping to prevent extreme values\n",
    "# df['spend'] = df['spend'].clip(lower=df['spend'].quantile(0.01), upper=df['spend'].quantile(0.99))\n",
    "# df['reach'] = df['reach'].clip(lower=df['reach'].quantile(0.01), upper=df['reach'].quantile(0.99))\n",
    "# df['engagement'] = df['engagement'].clip(lower=df['engagement'].quantile(0.01), upper=df['engagement'].quantile(0.99))\n",
    "\n",
    "# # debug: checking for NaN and inf values after clipping\n",
    "# print(\"NaN in df after clipping:\")\n",
    "# print(df[numerical_features + targets].isna().sum())\n",
    "# print(\"Inf in df after clipping:\")\n",
    "# print(np.isinf(df[numerical_features + targets]).sum())\n",
    "\n",
    "# # standardizing numerical features\n",
    "# feature_scaler = StandardScaler()\n",
    "# df[numerical_features] = feature_scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# # standardizing targets\n",
    "# target_scaler = StandardScaler()\n",
    "# df[targets] = target_scaler.fit_transform(df[targets])\n",
    "\n",
    "# # checking scaler parameters\n",
    "# print(\"Scaler mean:\", target_scaler.mean_)\n",
    "# print(\"Scaler scale:\", target_scaler.scale_)\n",
    "\n",
    "# # checking for NaN and inf values after scaling\n",
    "# print(\"NaN in df after scaling:\")\n",
    "# print(df[numerical_features + targets].isna().sum())\n",
    "# print(\"Inf in df after scaling:\")\n",
    "# print(np.isinf(df[numerical_features + targets]).sum())\n",
    "\n",
    "# # dropping rows with NaN or inf values (should be none at this point)\n",
    "# df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "# print(f\"Number of rows after dropping NaN/inf: {len(df)}\")\n",
    "\n",
    "# # storing unique campaigns before one-hot encoding\n",
    "# unique_campaigns = df['campaign'].unique()\n",
    "# print(\"\\nUnique campaigns:\")\n",
    "# print(unique_campaigns)\n",
    "\n",
    "# # One-hot encode categorical features with numeric dtype\n",
    "# df = pd.get_dummies(df, columns=categorical_features, dtype=np.float64)\n",
    "\n",
    "# # printing statistics after scaling to confirm\n",
    "# print(\"Feature stats after scaling:\\n\", df[numerical_features].describe())\n",
    "# print(\"Target stats after scaling:\\n\", df[targets].describe())\n",
    "\n",
    "# # shuffling the data before splitting\n",
    "# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # splitting the data\n",
    "# train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# # checking distribution of targets in each split\n",
    "# print(\"\\nTarget distribution in training set:\")\n",
    "# print(train_df[targets].describe())\n",
    "# print(\"\\nTarget distribution in validation set:\")\n",
    "# print(val_df[targets].describe())\n",
    "# print(\"\\nTarget distribution in test set:\")\n",
    "# print(test_df[targets].describe())\n",
    "\n",
    "# print(f\"\\nTraining samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in new features before filling:\n",
      "campaign_grouped     0\n",
      "month                0\n",
      "year                 0\n",
      "ad_type              0\n",
      "day_of_week          0\n",
      "is_holiday_period    0\n",
      "has_sale             0\n",
      "has_offer            0\n",
      "has_win              0\n",
      "dtype: int64\n",
      "\n",
      "NaN in new features after filling:\n",
      "campaign_grouped     0\n",
      "month                0\n",
      "year                 0\n",
      "ad_type              0\n",
      "day_of_week          0\n",
      "is_holiday_period    0\n",
      "has_sale             0\n",
      "has_offer            0\n",
      "has_win              0\n",
      "dtype: int64\n",
      "\n",
      "NaN in raw df:\n",
      "spend             0\n",
      "ad_name_length    0\n",
      "reach             0\n",
      "engagement        0\n",
      "dtype: int64\n",
      "Number of rows after dropping NaN: 22634\n",
      "\n",
      "Unique campaigns (first 5 for brevity):\n",
      "['Other' 'XBrand | Lifestyle Rewards | Auto Loans Special | Flash Sale'\n",
      " 'XBrand | Luxury Travel | Gold Loan Offers | Flash Sale'\n",
      " 'XBrand | Holiday Cashback | Auto Loans Special | Gold Loan Offers'\n",
      " 'XBrand | Diwali Deals | Lifestyle Rewards | Back to School']\n",
      "Total unique campaigns: 97\n",
      "Spend log-transformed quantiles (before scaling): 1st percentile: 1.4142, 99th percentile: 9.5700\n",
      "Scaler mean: [ 8.91405039 24.31046437]\n",
      "Scaler scale: [ 1.80411421 36.55105008]\n",
      "One-hot encoding successful.\n",
      "Feature stats after scaling:\n",
      "               spend  ad_name_length  is_holiday_period  has_sale  \\\n",
      "count  2.263400e+04    2.263400e+04       22634.000000   22634.0   \n",
      "mean   3.993153e-16    1.500572e-16           0.287223       0.0   \n",
      "std    1.000022e+00    1.000022e+00           0.452476       0.0   \n",
      "min   -4.074524e+00   -1.811144e+00           0.000000       0.0   \n",
      "25%   -5.216907e-01   -1.023576e+00           0.000000       0.0   \n",
      "50%    5.446324e-03    2.609682e-04           0.000000       0.0   \n",
      "75%    5.811282e-01    9.453416e-01           1.000000       0.0   \n",
      "max    2.281902e+00    1.732909e+00           1.000000       0.0   \n",
      "\n",
      "          has_offer       has_win  \n",
      "count  22634.000000  22634.000000  \n",
      "mean       0.324114      0.576920  \n",
      "std        0.468053      0.494059  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      0.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n",
      "\n",
      "Columns after one-hot encoding (first 10 for brevity):\n",
      "Index(['engagement', 'ad_name', 'campaign', 'date', 'reach', 'spend',\n",
      "       'is_holiday_period', 'ad_name_length', 'has_sale', 'has_offer'],\n",
      "      dtype='object')\n",
      "Total number of columns: 131\n",
      "\n",
      "Target distribution in training set:\n",
      "              reach    engagement\n",
      "count  18107.000000  18107.000000\n",
      "mean      -0.001249      0.003799\n",
      "std        0.999531      1.006034\n",
      "min       -3.519235     -0.665110\n",
      "25%       -0.540273     -0.539735\n",
      "50%        0.214939     -0.398447\n",
      "75%        0.667858      0.125466\n",
      "max        1.818292      9.234284\n",
      "\n",
      "Target distribution in validation set:\n",
      "             reach   engagement\n",
      "count  2263.000000  2263.000000\n",
      "mean     -0.010538    -0.044298\n",
      "std       0.997829     0.938864\n",
      "min      -3.519235    -0.665110\n",
      "25%      -0.559163    -0.539735\n",
      "50%       0.194957    -0.398447\n",
      "75%       0.670777     0.028104\n",
      "max       1.818292     9.234284\n",
      "\n",
      "Target distribution in test set:\n",
      "             reach   engagement\n",
      "count  2264.000000  2264.000000\n",
      "mean      0.020525     0.013893\n",
      "std       1.006290     1.010294\n",
      "min      -3.519235    -0.665110\n",
      "25%      -0.458581    -0.534622\n",
      "50%       0.226403    -0.382106\n",
      "75%       0.682846     0.206198\n",
      "max       1.818292     9.234284\n",
      "\n",
      "Training samples: 18107, Validation samples: 2263, Test samples: 2264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define holiday periods\n",
    "holiday_periods = {\n",
    "    'April': 'Avurudu',\n",
    "    'December': 'Christmas/New Year'\n",
    "}\n",
    "\n",
    "# Function to extract ad type from campaign name\n",
    "def extract_ad_type(campaign):\n",
    "    ad_types = ['Auction Reach', 'Auction Engagement', 'Lead Gen', 'Video Views', \n",
    "                'Auction Traffic', 'Auction Views', 'Link Clicks', 'Thruplays']\n",
    "    for ad_type in ad_types:\n",
    "        if ad_type in campaign:\n",
    "            return ad_type\n",
    "    return 'Other'\n",
    "\n",
    "# Group rare campaigns\n",
    "campaign_counts = df['campaign'].value_counts()\n",
    "threshold = 10  # Campaigns with fewer than 10 occurrences are grouped as 'Other'\n",
    "rare_campaigns = campaign_counts[campaign_counts < threshold].index\n",
    "df['campaign_grouped'] = df['campaign'].apply(lambda x: 'Other' if x in rare_campaigns else x)\n",
    "\n",
    "# Extract features from the date column\n",
    "df['month'] = df['date'].dt.strftime('%B')  # Full month name (e.g., January)\n",
    "df['year'] = df['date'].dt.year\n",
    "df['day_of_week'] = df['date'].dt.day_name()  # e.g., 'Monday'\n",
    "\n",
    "# Create is_holiday_period feature\n",
    "df['is_holiday_period'] = df['month'].apply(lambda x: 1 if x in holiday_periods else 0)\n",
    "\n",
    "# Extract ad_type from campaign\n",
    "df['ad_type'] = df['campaign'].apply(extract_ad_type)\n",
    "\n",
    "# Add text-based features from ad_name\n",
    "df['ad_name_length'] = df['ad_name'].str.len()\n",
    "df['has_sale'] = df['ad_name'].str.contains('Sale', case=False, na=False).astype(int)\n",
    "df['has_offer'] = df['ad_name'].str.contains('Offer', case=False, na=False).astype(int)\n",
    "df['has_win'] = df['ad_name'].str.contains('Win', case=False, na=False).astype(int)\n",
    "\n",
    "# Define features and targets\n",
    "numerical_features = ['spend', 'ad_name_length']\n",
    "binary_features = ['is_holiday_period', 'has_sale', 'has_offer', 'has_win']\n",
    "categorical_features = ['campaign_grouped', 'month', 'year', 'ad_type', 'day_of_week']\n",
    "targets = ['reach', 'engagement']\n",
    "\n",
    "# Debug: Check for NaN in new features\n",
    "print(\"NaN in new features before filling:\")\n",
    "print(df[categorical_features + binary_features].isna().sum())\n",
    "\n",
    "# Handle missing values in new features\n",
    "df['month'] = df['month'].fillna('Unknown')\n",
    "df['year'] = df['year'].fillna(df['year'].mode()[0] if not df['year'].isna().all() else 2023)\n",
    "df['day_of_week'] = df['day_of_week'].fillna('Unknown')\n",
    "df['ad_type'] = df['ad_type'].fillna('Unknown')\n",
    "df['campaign_grouped'] = df['campaign_grouped'].fillna('Unknown')\n",
    "df['ad_name_length'] = df['ad_name_length'].fillna(df['ad_name_length'].median())\n",
    "df['has_sale'] = df['has_sale'].fillna(0)\n",
    "df['has_offer'] = df['has_offer'].fillna(0)\n",
    "df['has_win'] = df['has_win'].fillna(0)\n",
    "\n",
    "# Debug: Check for NaN in new features after filling\n",
    "print(\"\\nNaN in new features after filling:\")\n",
    "print(df[categorical_features + binary_features].isna().sum())\n",
    "\n",
    "# Debug: Check for NaN in raw data\n",
    "print(\"\\nNaN in raw df:\")\n",
    "print(df[numerical_features + targets].isna().sum())\n",
    "\n",
    "# Drop rows with NaN values in numerical features or targets\n",
    "df = df.dropna(subset=numerical_features + targets)\n",
    "print(f\"Number of rows after dropping NaN: {len(df)}\")\n",
    "\n",
    "# Store unique campaigns before one-hot encoding\n",
    "unique_campaigns = df['campaign_grouped'].unique()\n",
    "print(\"\\nUnique campaigns (first 5 for brevity):\")\n",
    "print(unique_campaigns[:5])\n",
    "print(f\"Total unique campaigns: {len(unique_campaigns)}\")\n",
    "\n",
    "# Apply transformations to handle skewness\n",
    "df['spend'] = np.log1p(df['spend'])\n",
    "df['reach'] = np.log1p(df['reach'])\n",
    "df['engagement'] = np.sqrt(df['engagement'])  # Use sqrt for engagement\n",
    "\n",
    "# Store quantiles of spend in log-transformed space (before scaling)\n",
    "spend_log_lower = df['spend'].quantile(0.01)\n",
    "spend_log_upper = df['spend'].quantile(0.99)\n",
    "print(f\"Spend log-transformed quantiles (before scaling): 1st percentile: {spend_log_lower:.4f}, 99th percentile: {spend_log_upper:.4f}\")\n",
    "\n",
    "# Clip outliers\n",
    "df['spend'] = df['spend'].clip(lower=spend_log_lower, upper=spend_log_upper)\n",
    "df['reach'] = df['reach'].clip(lower=df['reach'].quantile(0.01), upper=df['reach'].quantile(0.99))\n",
    "df['engagement'] = df['engagement'].clip(lower=df['engagement'].quantile(0.01), upper=df['engagement'].quantile(0.999))\n",
    "\n",
    "# Standardize numerical features\n",
    "feature_scaler = StandardScaler()\n",
    "df[numerical_features] = feature_scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Standardize binary features\n",
    "df[binary_features] = df[binary_features].astype(float)\n",
    "\n",
    "# Standardize targets\n",
    "target_scaler = StandardScaler()\n",
    "df[targets] = target_scaler.fit_transform(df[targets])\n",
    "\n",
    "# Debug: Check scaler parameters\n",
    "print(\"Scaler mean:\", target_scaler.mean_)\n",
    "print(\"Scaler scale:\", target_scaler.scale_)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "try:\n",
    "    df = pd.get_dummies(df, columns=categorical_features, dtype=np.float64)\n",
    "    print(\"One-hot encoding successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during one-hot encoding: {e}\")\n",
    "    raise\n",
    "\n",
    "# Print statistics after scaling\n",
    "print(\"Feature stats after scaling:\\n\", df[numerical_features + binary_features].describe())\n",
    "\n",
    "# Debug: Check column names after one-hot encoding\n",
    "print(\"\\nColumns after one-hot encoding (first 10 for brevity):\")\n",
    "print(df.columns[:10])\n",
    "print(f\"Total number of columns: {len(df.columns)}\")\n",
    "\n",
    "# Shuffle the data before splitting\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Debug: Check distribution of targets in each split\n",
    "print(\"\\nTarget distribution in training set:\")\n",
    "print(train_df[targets].describe())\n",
    "print(\"\\nTarget distribution in validation set:\")\n",
    "print(val_df[targets].describe())\n",
    "print(\"\\nTarget distribution in test set:\")\n",
    "print(test_df[targets].describe())\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtype: float64\n",
      "y_train dtype: float64\n",
      "X_train shape: (17478, 216)\n",
      "y_train shape: (17478, 2)\n",
      "X_val shape: (2185, 216)\n",
      "y_val shape: (2185, 2)\n",
      "X_test shape: (2185, 216)\n",
      "y_test shape: (2185, 2)\n"
     ]
    }
   ],
   "source": [
    "# # preparing input features (numerical + one-hot encoded campaign)\n",
    "# campaign_columns = [col for col in train_df.columns if col.startswith('campaign_')]\n",
    "# feature_columns = numerical_features + campaign_columns\n",
    "\n",
    "# # converting to NumPy arrays and ensure numeric type\n",
    "# X_train = train_df[feature_columns].values.astype(np.float64)\n",
    "# y_train = train_df[targets].values.astype(np.float64)\n",
    "# X_val = val_df[feature_columns].values.astype(np.float64)\n",
    "# y_val = val_df[targets].values.astype(np.float64)\n",
    "# X_test = test_df[feature_columns].values.astype(np.float64)\n",
    "# y_test = test_df[targets].values.astype(np.float64)\n",
    "\n",
    "# # checking dtypes\n",
    "# print(\"X_train dtype:\", X_train.dtype)\n",
    "# print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "# # printing shapes to confirm\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_val shape:\", X_val.shape)\n",
    "# print(\"y_val shape:\", y_val.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtype: float64\n",
      "y_train dtype: float64\n",
      "X_train shape: (18107, 126)\n",
      "y_train shape: (18107, 2)\n",
      "X_val shape: (2263, 126)\n",
      "y_val shape: (2263, 2)\n",
      "X_test shape: (2264, 126)\n",
      "y_test shape: (2264, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input features (numerical + binary + one-hot encoded categorical)\n",
    "campaign_columns = [col for col in train_df.columns if col.startswith('campaign_grouped_')]\n",
    "month_columns = [col for col in train_df.columns if col.startswith('month_')]\n",
    "year_columns = [col for col in train_df.columns if col.startswith('year_')]\n",
    "ad_type_columns = [col for col in train_df.columns if col.startswith('ad_type_')]\n",
    "day_of_week_columns = [col for col in train_df.columns if col.startswith('day_of_week_')]\n",
    "feature_columns = numerical_features + binary_features + campaign_columns + month_columns + year_columns + ad_type_columns + day_of_week_columns\n",
    "\n",
    "# Convert to NumPy arrays and ensure numeric type\n",
    "X_train = train_df[feature_columns].values.astype(np.float64)\n",
    "y_train = train_df[targets].values.astype(np.float64)\n",
    "X_val = val_df[feature_columns].values.astype(np.float64)\n",
    "y_val = val_df[targets].values.astype(np.float64)\n",
    "X_test = test_df[feature_columns].values.astype(np.float64)\n",
    "y_test = test_df[targets].values.astype(np.float64)\n",
    "\n",
    "# Check dtypes\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Train Loss: 0.9652, Val Loss: 0.9176\n",
      "Epoch [20/200], Train Loss: 0.8431, Val Loss: 0.7984\n",
      "Epoch [30/200], Train Loss: 0.6376, Val Loss: 0.6146\n",
      "Epoch [40/200], Train Loss: 0.5033, Val Loss: 0.4950\n",
      "Epoch [50/200], Train Loss: 0.4309, Val Loss: 0.4215\n",
      "Epoch [60/200], Train Loss: 0.3671, Val Loss: 0.3567\n",
      "Epoch [70/200], Train Loss: 0.2943, Val Loss: 0.2778\n",
      "Epoch [80/200], Train Loss: 0.2200, Val Loss: 0.2012\n",
      "Epoch [90/200], Train Loss: 0.2000, Val Loss: 0.1831\n",
      "Epoch [100/200], Train Loss: 0.1854, Val Loss: 0.1710\n",
      "Epoch [110/200], Train Loss: 0.1794, Val Loss: 0.1672\n",
      "Epoch [120/200], Train Loss: 0.1755, Val Loss: 0.1635\n",
      "Epoch [130/200], Train Loss: 0.1723, Val Loss: 0.1619\n",
      "Epoch [140/200], Train Loss: 0.1705, Val Loss: 0.1606\n",
      "Epoch [150/200], Train Loss: 0.1685, Val Loss: 0.1594\n",
      "Epoch [160/200], Train Loss: 0.1674, Val Loss: 0.1584\n",
      "Epoch [170/200], Train Loss: 0.1660, Val Loss: 0.1575\n",
      "Epoch [180/200], Train Loss: 0.1647, Val Loss: 0.1568\n",
      "Epoch [190/200], Train Loss: 0.1640, Val Loss: 0.1561\n",
      "Epoch [200/200], Train Loss: 0.1625, Val Loss: 0.1555\n",
      "Model saved to 'mlp_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # defining the MLP model\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 32)\n",
    "#         self.fc4 = nn.Linear(32, 2)  # output will be reach and engagement\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.1)  # reducing dropout rate\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.fc3(x))\n",
    "#         x = self.fc4(x)\n",
    "#         return x\n",
    "\n",
    "# # initializing the model\n",
    "# input_dim = X_train.shape[1]  # number of features (216)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = MLP(input_dim).to(device)\n",
    "\n",
    "# # defining loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# # converting data to PyTorch tensors\n",
    "# X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "# y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "# X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "# y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "# # training loop with early stopping\n",
    "# num_epochs = 200\n",
    "# patience = 20\n",
    "# best_val_loss = float('inf')\n",
    "# epochs_no_improve = 0\n",
    "# best_model_state = None\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_outputs = model(X_val_tensor)\n",
    "#         val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "#     # early stopping\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         epochs_no_improve = 0\n",
    "#         best_model_state = model.state_dict()\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "\n",
    "#     if epochs_no_improve >= patience:\n",
    "#         print(f'Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss.item():.4f}')\n",
    "#         break\n",
    "\n",
    "# # loading the best model\n",
    "# model.load_state_dict(best_model_state)\n",
    "\n",
    "# # saving the model\n",
    "# torch.save(model.state_dict(), 'mlp_model.pth')\n",
    "# print(\"Model saved to 'mlp_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Train Loss: 0.9666, Val Loss: 0.8959\n",
      "Epoch [20/200], Train Loss: 0.8472, Val Loss: 0.7694\n",
      "Epoch [30/200], Train Loss: 0.6161, Val Loss: 0.5538\n",
      "Epoch [40/200], Train Loss: 0.5390, Val Loss: 0.5074\n",
      "Epoch [50/200], Train Loss: 0.4898, Val Loss: 0.4645\n",
      "Epoch [60/200], Train Loss: 0.4576, Val Loss: 0.4487\n",
      "Epoch [70/200], Train Loss: 0.4405, Val Loss: 0.4368\n",
      "Epoch [80/200], Train Loss: 0.4294, Val Loss: 0.4312\n",
      "Epoch [90/200], Train Loss: 0.4221, Val Loss: 0.4267\n",
      "Epoch [100/200], Train Loss: 0.4125, Val Loss: 0.4228\n",
      "Epoch [110/200], Train Loss: 0.4047, Val Loss: 0.4189\n",
      "Epoch [120/200], Train Loss: 0.3966, Val Loss: 0.4152\n",
      "Epoch [130/200], Train Loss: 0.3941, Val Loss: 0.4121\n",
      "Epoch [140/200], Train Loss: 0.3888, Val Loss: 0.4098\n",
      "Epoch [150/200], Train Loss: 0.3821, Val Loss: 0.4069\n",
      "Epoch [160/200], Train Loss: 0.3823, Val Loss: 0.4042\n",
      "Epoch [170/200], Train Loss: 0.3757, Val Loss: 0.4023\n",
      "Epoch [180/200], Train Loss: 0.3738, Val Loss: 0.4011\n",
      "Epoch [190/200], Train Loss: 0.3690, Val Loss: 0.3991\n",
      "Epoch [200/200], Train Loss: 0.3669, Val Loss: 0.3979\n",
      "Model saved to 'mlp_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Defining the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)  # Output will be reach and engagement\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)  # Reducing dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initializing the model\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(input_dim).to(device)\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss.item():.4f}')\n",
    "        break\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'mlp_model.pth')\n",
    "print(\"Model saved to 'mlp_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Reach (scaled): 0.3185\n",
      "RMSE for Engagement (scaled): 0.4608\n",
      "RMSE for Reach (original scale): 11011.15\n",
      "RMSE for Engagement (original scale): 2686.57\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # loading the model\n",
    "# model.load_state_dict(torch.load('mlp_model.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # converting test data to PyTorch tensors\n",
    "# X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "# y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# # predicting on test set\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model(X_test_tensor).cpu().numpy()\n",
    "#     y_test_np = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# # calculating RMSE in scaled space\n",
    "# rmse_reach_scaled = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred[:, 0]))\n",
    "# rmse_engagement_scaled = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred[:, 1]))\n",
    "# print(f'RMSE for Reach (scaled): {rmse_reach_scaled:.4f}')\n",
    "# print(f'RMSE for Engagement (scaled): {rmse_engagement_scaled:.4f}')\n",
    "\n",
    "# # inverse transform to original scale\n",
    "# y_test_original = target_scaler.inverse_transform(y_test_np)\n",
    "# y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "\n",
    "# # undo log transformation\n",
    "# y_test_original = np.expm1(y_test_original)\n",
    "# y_pred_original = np.expm1(y_pred_original)\n",
    "\n",
    "# # calculating RMSE in original scale\n",
    "# rmse_reach_original = np.sqrt(mean_squared_error(y_test_original[:, 0], y_pred_original[:, 0]))\n",
    "# rmse_engagement_original = np.sqrt(mean_squared_error(y_test_original[:, 1], y_pred_original[:, 1]))\n",
    "# print(f'RMSE for Reach (original scale): {rmse_reach_original:.2f}')\n",
    "# print(f'RMSE for Engagement (original scale): {rmse_engagement_original:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Reach (scaled): 0.5675\n",
      "RMSE for Engagement (scaled): 0.6711\n",
      "RMSE for Reach (original scale): 22188.72\n",
      "RMSE for Engagement (original scale): 6182.10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Loading the model\n",
    "model.load_state_dict(torch.load('mlp_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Converting test data to PyTorch tensors\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Predicting on test set\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).cpu().numpy()\n",
    "    y_test_np = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Calculating RMSE in scaled space\n",
    "rmse_reach_scaled = np.sqrt(mean_squared_error(y_test_np[:, 0], y_pred[:, 0]))\n",
    "rmse_engagement_scaled = np.sqrt(mean_squared_error(y_test_np[:, 1], y_pred[:, 1]))\n",
    "print(f'RMSE for Reach (scaled): {rmse_reach_scaled:.4f}')\n",
    "print(f'RMSE for Engagement (scaled): {rmse_engagement_scaled:.4f}')\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_test_original = target_scaler.inverse_transform(y_test_np)\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Undo transformations\n",
    "y_test_original[:, 0] = np.expm1(y_test_original[:, 0])  # Undo log for reach\n",
    "y_test_original[:, 1] = y_test_original[:, 1] ** 2  # Undo sqrt for engagement\n",
    "y_test_original = np.maximum(y_test_original, 0)\n",
    "\n",
    "y_pred_original[:, 0] = np.expm1(y_pred_original[:, 0])  # Undo log for reach\n",
    "y_pred_original[:, 1] = y_pred_original[:, 1] ** 2  # Undo sqrt for engagement\n",
    "y_pred_original = np.maximum(y_pred_original, 0)\n",
    "\n",
    "# Calculating RMSE in original scale\n",
    "rmse_reach_original = np.sqrt(mean_squared_error(y_test_original[:, 0], y_pred_original[:, 0]))\n",
    "rmse_engagement_original = np.sqrt(mean_squared_error(y_test_original[:, 1], y_pred_original[:, 1]))\n",
    "print(f'RMSE for Reach (original scale): {rmse_reach_original:.2f}')\n",
    "print(f'RMSE for Engagement (original scale): {rmse_engagement_original:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Reach: 46848.78\n",
      "Predicted Engagement: 3506.437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Hardcode the log-transformed quantiles from Cell 2\n",
    "spend_log_lower = 1.4142  # From Cell 2 output\n",
    "spend_log_upper = 9.5700  # From Cell 2 output\n",
    "\n",
    "# Create new data point\n",
    "campaign_name = 'XBrand | Green Finance | Holiday Cashback | Savings Fiesta'\n",
    "ad_name = 'XBrand | Save More | Zero Interest | Easy Payments'\n",
    "date_str = '2023-05-01'\n",
    "date = pd.to_datetime(date_str, errors='coerce')\n",
    "\n",
    "# Extract features\n",
    "month = date.strftime('%B')\n",
    "year = date.year\n",
    "day_of_week = date.day_name()\n",
    "is_holiday_period = 1 if month in holiday_periods else 0\n",
    "\n",
    "# Extract ad_type\n",
    "def extract_ad_type(campaign):\n",
    "    ad_types = ['Auction Reach', 'Auction Engagement', 'Lead Gen', 'Video Views', \n",
    "                'Auction Traffic', 'Auction Views', 'Link Clicks', 'Thruplays']\n",
    "    for ad_type in ad_types:\n",
    "        if ad_type in campaign:\n",
    "            return ad_type\n",
    "    return 'Other'\n",
    "\n",
    "ad_type = extract_ad_type(campaign_name)\n",
    "\n",
    "# Group campaign (same logic as in Cell 2)\n",
    "campaign_grouped = 'Other' if campaign_name in rare_campaigns else campaign_name\n",
    "\n",
    "# Extract ad_name features\n",
    "ad_name_length = len(ad_name)\n",
    "has_sale = 1 if 'Sale' in ad_name else 0\n",
    "has_offer = 1 if 'Offer' in ad_name else 0\n",
    "has_win = 1 if 'Win' in ad_name else 0\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'spend': [4000],\n",
    "    'campaign_grouped': [campaign_grouped],\n",
    "    'month': [month],\n",
    "    'year': [year],\n",
    "    'day_of_week': [day_of_week],\n",
    "    'ad_type': [ad_type],\n",
    "    'is_holiday_period': [is_holiday_period],\n",
    "    'ad_name_length': [ad_name_length],\n",
    "    'has_sale': [has_sale],\n",
    "    'has_offer': [has_offer],\n",
    "    'has_win': [has_win]\n",
    "})\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data['spend'] = np.log1p(new_data['spend'])\n",
    "new_data['spend'] = new_data['spend'].clip(lower=spend_log_lower, upper=spend_log_upper)\n",
    "new_data[['spend', 'ad_name_length']] = feature_scaler.transform(new_data[['spend', 'ad_name_length']])\n",
    "new_data[['is_holiday_period', 'has_sale', 'has_offer', 'has_win']] = new_data[['is_holiday_period', 'has_sale', 'has_offer', 'has_win']].astype(float)\n",
    "new_data = pd.get_dummies(new_data, columns=['campaign_grouped', 'month', 'year', 'ad_type', 'day_of_week'], dtype=np.float64)\n",
    "new_data = new_data.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_new_tensor = torch.FloatTensor(new_data.values).to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_new_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "predictions = target_scaler.inverse_transform(predictions)\n",
    "predictions[:, 0] = np.expm1(predictions[:, 0])  # Undo log for reach\n",
    "predictions[:, 1] = predictions[:, 1] ** 2  # Undo sqrt for engagement\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "print('Predicted Reach:', predictions[0, 0])\n",
    "print('Predicted Engagement:', predictions[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using campaign: HNB | May Monthly Boosting | Auction Reach | RO 63711 | May 2023\n",
      "Predicted Reach: 100564.664\n",
      "Predicted Engagement: 1516.6621\n"
     ]
    }
   ],
   "source": [
    "# # Hardcode the log-transformed quantiles from Step 2\n",
    "# spend_log_lower = 2.5705\n",
    "# spend_log_upper = 9.5649\n",
    "\n",
    "# # Create new data point\n",
    "# # Use a different campaign name from the unique campaigns list\n",
    "# campaign_name = 'HNB | May Monthly Boosting | Auction Reach | RO 63711 | May 2023'  # Different campaign\n",
    "# print(f\"Using campaign: {campaign_name}\")\n",
    "\n",
    "# new_data = pd.DataFrame({\n",
    "#     'spend': [4000],\n",
    "#     'campaign': [campaign_name]\n",
    "# })\n",
    "\n",
    "# # Preprocess the new data\n",
    "# new_data['spend'] = np.log1p(new_data['spend'])\n",
    "# # Clip using the log-transformed quantiles (before scaling)\n",
    "# new_data['spend'] = new_data['spend'].clip(lower=spend_log_lower, upper=spend_log_upper)\n",
    "# new_data[['spend']] = feature_scaler.transform(new_data[['spend']])\n",
    "# new_data = pd.get_dummies(new_data, columns=['campaign'], dtype=np.float64)\n",
    "# new_data = new_data.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "# # Convert to PyTorch tensor\n",
    "# X_new_tensor = torch.FloatTensor(new_data.values).to(device)\n",
    "\n",
    "# # Predict\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_new_tensor).cpu().numpy()\n",
    "\n",
    "# # Inverse transform predictions\n",
    "# predictions = target_scaler.inverse_transform(predictions)\n",
    "# predictions = np.expm1(predictions)\n",
    "\n",
    "# # Ensure predictions are non-negative\n",
    "# predictions = np.maximum(predictions, 0)\n",
    "\n",
    "# print('Predicted Reach:', predictions[0, 0])\n",
    "# print('Predicted Engagement:', predictions[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model predictions on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics on Test Set:\n",
      "RMSE Reach (original scale): 11011.15\n",
      "RMSE Engagement (original scale): 2686.57\n",
      "MAE Reach: 4910.43\n",
      "MAE Engagement: 813.58\n",
      "MAPE Reach: 49.55%\n",
      "MAPE Engagement: 159.68%\n",
      "R² Reach: 0.8853\n",
      "R² Engagement: 0.6357\n",
      "\n",
      "Sample of Predictions vs Actual Values (first 10 rows):\n",
      "   Actual Reach  Predicted Reach  Actual Engagement  Predicted Engagement\n",
      "0       16764.0     15044.733398               13.0             19.662148\n",
      "1       14478.0     13532.988281             1308.0           1238.735718\n",
      "2         744.0      1523.711914               96.0            360.638489\n",
      "3        6011.0      5187.545898               10.0             20.935696\n",
      "4       19383.0     19902.220703              755.0             56.206776\n",
      "5       52923.0     65795.703125            19190.0          27014.277344\n",
      "6       68284.0     18862.894531             2379.0            709.759277\n",
      "7        2693.0      5309.661621               14.0             14.991861\n",
      "8       35089.0     21981.626953              532.0            213.748840\n",
      "9         778.0       735.685730               78.0             53.634296\n",
      "\n",
      "Summary of Errors:\n",
      "         Reach Error  Engagement Error\n",
      "count    2185.000000       2185.000000\n",
      "mean    -1803.771924       -369.808265\n",
      "std     10864.890120       2661.607724\n",
      "min   -167474.059883     -23918.007357\n",
      "25%     -2278.729492        -93.017761\n",
      "50%       -89.087646          1.161835\n",
      "75%      1165.361328         39.592403\n",
      "max     51454.015625      27044.236295\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# # Convert test data to PyTorch tensor\n",
    "# X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "# y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# # Predict on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# # Inverse transform predictions and actual values to original scale\n",
    "# y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "# y_pred_original = np.expm1(y_pred_original)\n",
    "# y_pred_original = np.maximum(y_pred_original, 0)  # Ensure non-negative\n",
    "\n",
    "# y_test_original = target_scaler.inverse_transform(y_test)\n",
    "# y_test_original = np.expm1(y_test_original)\n",
    "# y_test_original = np.maximum(y_test_original, 0)  # Ensure non-negative\n",
    "\n",
    "# # Create a DataFrame to compare predictions and actual values\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Actual Reach': y_test_original[:, 0],\n",
    "#     'Predicted Reach': y_pred_original[:, 0],\n",
    "#     'Actual Engagement': y_test_original[:, 1],\n",
    "#     'Predicted Engagement': y_pred_original[:, 1]\n",
    "# })\n",
    "\n",
    "# # Compute additional metrics\n",
    "# # Mean Absolute Error (MAE)\n",
    "# mae_reach = mean_absolute_error(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])\n",
    "# mae_engagement = mean_absolute_error(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])\n",
    "\n",
    "# # Mean Absolute Percentage Error (MAPE)\n",
    "# # Add small epsilon to avoid division by zero\n",
    "# epsilon = 1e-10\n",
    "# mape_reach = np.mean(np.abs((comparison_df['Actual Reach'] - comparison_df['Predicted Reach']) / (comparison_df['Actual Reach'] + epsilon))) * 100\n",
    "# mape_engagement = np.mean(np.abs((comparison_df['Actual Engagement'] - comparison_df['Predicted Engagement']) / (comparison_df['Actual Engagement'] + epsilon))) * 100\n",
    "\n",
    "# # R² Score\n",
    "# r2_reach = r2_score(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])\n",
    "# r2_engagement = r2_score(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])\n",
    "\n",
    "# # Print evaluation metrics\n",
    "# print(\"\\nEvaluation Metrics on Test Set:\")\n",
    "# print(f\"RMSE Reach (original scale): {np.sqrt(mean_squared_error(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])):.2f}\")\n",
    "# print(f\"RMSE Engagement (original scale): {np.sqrt(mean_squared_error(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])):.2f}\")\n",
    "# print(f\"MAE Reach: {mae_reach:.2f}\")\n",
    "# print(f\"MAE Engagement: {mae_engagement:.2f}\")\n",
    "# print(f\"MAPE Reach: {mape_reach:.2f}%\")\n",
    "# print(f\"MAPE Engagement: {mape_engagement:.2f}%\")\n",
    "# print(f\"R² Reach: {r2_reach:.4f}\")\n",
    "# print(f\"R² Engagement: {r2_engagement:.4f}\")\n",
    "\n",
    "# # Display a sample of predictions vs actual values\n",
    "# print(\"\\nSample of Predictions vs Actual Values (first 10 rows):\")\n",
    "# print(comparison_df.head(10))\n",
    "\n",
    "# # Summary statistics of errors\n",
    "# comparison_df['Reach Error'] = comparison_df['Predicted Reach'] - comparison_df['Actual Reach']\n",
    "# comparison_df['Engagement Error'] = comparison_df['Predicted Engagement'] - comparison_df['Actual Engagement']\n",
    "# print(\"\\nSummary of Errors:\")\n",
    "# print(comparison_df[['Reach Error', 'Engagement Error']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics on Test Set:\n",
      "RMSE Reach (original scale): 22188.72\n",
      "RMSE Engagement (original scale): 6182.10\n",
      "MAE Reach: 11450.66\n",
      "MAE Engagement: 1506.74\n",
      "MAPE Reach: 147.93%\n",
      "MAPE Engagement: 958714232710.47%\n",
      "R² Reach: 0.5591\n",
      "R² Engagement: 0.4346\n",
      "\n",
      "Sample of Predictions vs Actual Values (first 10 rows):\n",
      "    Actual Reach  Predicted Reach  Actual Engagement  Predicted Engagement\n",
      "0    7540.000000      5361.107910               10.0            170.970901\n",
      "1    1157.000000      2931.041504               50.0            116.877968\n",
      "2   45575.000000     33812.152344               87.0            412.119781\n",
      "3    6013.000000     12503.794922              270.0            466.499451\n",
      "4   39235.000000     29855.482422               59.0            237.073441\n",
      "5    5482.000000     20806.177734               41.0            630.929565\n",
      "6   30641.000000     26691.087891                7.0            515.598328\n",
      "7   43569.000000     57143.621094              433.0          17435.679688\n",
      "8  197689.332763    110090.375000            16290.0          15299.702148\n",
      "9    2302.000000      2896.488525               67.0             29.137604\n",
      "\n",
      "Summary of Errors:\n",
      "         Reach Error  Engagement Error\n",
      "count    2264.000000       2264.000000\n",
      "mean    -5761.280109       -677.740734\n",
      "std     21432.444710       6146.199484\n",
      "min   -161335.848388    -107125.652334\n",
      "25%     -8000.022156       -261.071778\n",
      "50%     -1273.041016         35.273212\n",
      "75%      2478.703918        211.109524\n",
      "max    104567.421875      42617.945312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Convert test data to PyTorch tensor\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions and actual values to original scale\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "y_pred_original[:, 0] = np.expm1(y_pred_original[:, 0])  # Undo log for reach\n",
    "y_pred_original[:, 1] = y_pred_original[:, 1] ** 2  # Undo sqrt for engagement\n",
    "y_pred_original = np.maximum(y_pred_original, 0)\n",
    "\n",
    "y_test_original = target_scaler.inverse_transform(y_test)\n",
    "y_test_original[:, 0] = np.expm1(y_test_original[:, 0])  # Undo log for reach\n",
    "y_test_original[:, 1] = y_test_original[:, 1] ** 2  # Undo sqrt for engagement\n",
    "y_test_original = np.maximum(y_test_original, 0)\n",
    "\n",
    "# Create a DataFrame to compare predictions and actual values\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual Reach': y_test_original[:, 0],\n",
    "    'Predicted Reach': y_pred_original[:, 0],\n",
    "    'Actual Engagement': y_test_original[:, 1],\n",
    "    'Predicted Engagement': y_pred_original[:, 1]\n",
    "})\n",
    "\n",
    "# Compute additional metrics\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_reach = mean_absolute_error(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])\n",
    "mae_engagement = mean_absolute_error(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "epsilon = 1e-10\n",
    "mape_reach = np.mean(np.abs((comparison_df['Actual Reach'] - comparison_df['Predicted Reach']) / (comparison_df['Actual Reach'] + epsilon))) * 100\n",
    "mape_engagement = np.mean(np.abs((comparison_df['Actual Engagement'] - comparison_df['Predicted Engagement']) / (comparison_df['Actual Engagement'] + epsilon))) * 100\n",
    "\n",
    "# R² Score\n",
    "r2_reach = r2_score(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])\n",
    "r2_engagement = r2_score(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nEvaluation Metrics on Test Set:\")\n",
    "print(f\"RMSE Reach (original scale): {np.sqrt(mean_squared_error(comparison_df['Actual Reach'], comparison_df['Predicted Reach'])):.2f}\")\n",
    "print(f\"RMSE Engagement (original scale): {np.sqrt(mean_squared_error(comparison_df['Actual Engagement'], comparison_df['Predicted Engagement'])):.2f}\")\n",
    "print(f\"MAE Reach: {mae_reach:.2f}\")\n",
    "print(f\"MAE Engagement: {mae_engagement:.2f}\")\n",
    "print(f\"MAPE Reach: {mape_reach:.2f}%\")\n",
    "print(f\"MAPE Engagement: {mape_engagement:.2f}%\")\n",
    "print(f\"R² Reach: {r2_reach:.4f}\")\n",
    "print(f\"R² Engagement: {r2_engagement:.4f}\")\n",
    "\n",
    "# Display a sample of predictions vs actual values\n",
    "print(\"\\nSample of Predictions vs Actual Values (first 10 rows):\")\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "# Summary statistics of errors\n",
    "comparison_df['Reach Error'] = comparison_df['Predicted Reach'] - comparison_df['Actual Reach']\n",
    "comparison_df['Engagement Error'] = comparison_df['Predicted Engagement'] - comparison_df['Actual Engagement']\n",
    "print(\"\\nSummary of Errors:\")\n",
    "print(comparison_df[['Reach Error', 'Engagement Error']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct Test Set and Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6 Prediction (Spend=4000, Campaign=HNB | Credit Cards | April Offers 2023 | Auction Reach | RO 64550 | March 2023):\n",
      "Predicted Reach: 108745.32, Predicted Engagement: 10683.11\n",
      "\n",
      "Data Points with the Same Campaign:\n",
      "      Spend                                           Campaign  Actual Reach  \\\n",
      "104  689.16  HNB | Credit Cards | April Offers 2023 | Aucti...       33192.0   \n",
      "\n",
      "     Predicted Reach  Actual Engagement  Predicted Engagement  \n",
      "104     36044.199219             1996.0           2240.762695  \n",
      "\n",
      "Data Points with Similar Spend (3500 ≤ Spend ≤ 4500):\n",
      "        Spend                                           Campaign  \\\n",
      "8     3598.74  HNB | February Monthly Boosting 2025 | Digital...   \n",
      "26    3851.34  HNB | Avrudu Meta Content Advertising 2024 | A...   \n",
      "32    3943.09  HNB | Digital Banking Campaign Meta | Auction ...   \n",
      "60    3737.63  HNB | January Monthly Boosting 2025 | Corporat...   \n",
      "97    4089.66  HNB | October Monthly Boosting | Auction Reach...   \n",
      "...       ...                                                ...   \n",
      "2017  3997.15  HNB | Stamp Launch Video | Auction Views | RO ...   \n",
      "2054  4140.98  HNB | Remittance Campaign Launch Reciever S&T ...   \n",
      "2166  3504.05  HNB | May Monthly Boosting | Auction Reach | R...   \n",
      "2174  3526.80  HNB | August Monthly Boosting | Auction Engage...   \n",
      "2182  3507.88  HNB | Meta April Monthly Boosting | Auction En...   \n",
      "\n",
      "      Actual Reach  Predicted Reach  Actual Engagement  Predicted Engagement  \n",
      "8          35089.0     21981.626953              532.0            213.748840  \n",
      "26         22416.0     24124.033203             2083.0           2136.049561  \n",
      "32         15218.0     22836.289062             4551.0           8206.276367  \n",
      "60        133826.0     91270.218750              473.0            547.379333  \n",
      "97        142788.0     68579.414062            24513.0           1585.350952  \n",
      "...            ...              ...                ...                   ...  \n",
      "2017       12972.0     11686.618164             4308.0           4903.613281  \n",
      "2054       20829.0     13430.983398            17539.0           5997.562012  \n",
      "2166       91127.0     62186.214844              120.0            528.609314  \n",
      "2174       10882.0     17089.105469              944.0           2855.936523  \n",
      "2182       42842.0     28626.992188            24717.0           4848.983398  \n",
      "\n",
      "[63 rows x 6 columns]\n",
      "\n",
      "Summary Statistics for Similar Spend Data Points:\n",
      "        Actual Reach  Actual Engagement\n",
      "count      63.000000          63.000000\n",
      "mean    53469.000000        7216.521733\n",
      "std     36166.696698        8773.791715\n",
      "min      7608.000000          62.000000\n",
      "25%     22691.500000         337.000000\n",
      "50%     50034.000000        3196.000000\n",
      "75%     75256.500000       12323.500000\n",
      "max    157364.000000       29758.869174\n"
     ]
    }
   ],
   "source": [
    "# # Reconstruct the original test set features\n",
    "# # Inverse transform the scaled spend\n",
    "# spend_scaled = X_test[:, 0]  # First column is scaled spend\n",
    "# spend_log = feature_scaler.inverse_transform(spend_scaled.reshape(-1, 1)).flatten()\n",
    "# spend_original = np.expm1(spend_log)\n",
    "\n",
    "# # Reconstruct the campaign names from one-hot encoded columns\n",
    "# campaign_columns = [col for col in feature_columns if col.startswith('campaign_')]\n",
    "# campaign_indices = np.argmax(X_test[:, 1:], axis=1)  # Index of the 1 in one-hot encoding\n",
    "# campaign_names = [campaign_columns[idx].replace('campaign_', '') for idx in campaign_indices]\n",
    "\n",
    "# # Create a DataFrame with the original test set features and actual/predicted values\n",
    "# test_df_reconstructed = pd.DataFrame({\n",
    "#     'Spend': spend_original,\n",
    "#     'Campaign': campaign_names,\n",
    "#     'Actual Reach': y_test_original[:, 0],\n",
    "#     'Predicted Reach': y_pred_original[:, 0],\n",
    "#     'Actual Engagement': y_test_original[:, 1],\n",
    "#     'Predicted Engagement': y_pred_original[:, 1]\n",
    "# })\n",
    "\n",
    "# # Step 6 prediction for comparison\n",
    "# step6_pred_reach = 108745.32\n",
    "# step6_pred_engagement = 10683.112\n",
    "# step6_spend = 4000\n",
    "# step6_campaign = 'HNB | Credit Cards | April Offers 2023 | Auction Reach | RO 64550 | March 2023'\n",
    "\n",
    "# # Find data points with the same campaign\n",
    "# same_campaign_df = test_df_reconstructed[\n",
    "#     test_df_reconstructed['Campaign'] == step6_campaign\n",
    "# ]\n",
    "\n",
    "# # Find data points with similar spend (within ±500 of 4000)\n",
    "# similar_spend_df = test_df_reconstructed[\n",
    "#     (test_df_reconstructed['Spend'] >= 3500) & (test_df_reconstructed['Spend'] <= 4500)\n",
    "# ]\n",
    "\n",
    "# # Print results\n",
    "# print(f\"\\nStep 6 Prediction (Spend={step6_spend}, Campaign={step6_campaign}):\")\n",
    "# print(f\"Predicted Reach: {step6_pred_reach:.2f}, Predicted Engagement: {step6_pred_engagement:.2f}\")\n",
    "\n",
    "# print(\"\\nData Points with the Same Campaign:\")\n",
    "# if not same_campaign_df.empty:\n",
    "#     print(same_campaign_df)\n",
    "# else:\n",
    "#     print(\"No data points found with the same campaign in the test set.\")\n",
    "\n",
    "# print(\"\\nData Points with Similar Spend (3500 ≤ Spend ≤ 4500):\")\n",
    "# if not similar_spend_df.empty:\n",
    "#     print(similar_spend_df)\n",
    "#     print(\"\\nSummary Statistics for Similar Spend Data Points:\")\n",
    "#     print(similar_spend_df[['Actual Reach', 'Actual Engagement']].describe())\n",
    "# else:\n",
    "#     print(\"No data points found with similar spend in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6 Prediction (Spend=4000, Campaign=XBrand | Green Finance | Holiday Cashback | Savings Fiesta):\n",
      "Predicted Reach: 46848.78, Predicted Engagement: 3506.44\n",
      "\n",
      "Data Points with the Same Campaign:\n",
      "No data points found with the same campaign in the test set.\n",
      "\n",
      "Data Points with Similar Spend (3500 ≤ Spend ≤ 4500):\n",
      "        Spend Campaign      Month  Year Ad Type Day of Week  \\\n",
      "28    3710.10    Other    January  2025   Other     Tuesday   \n",
      "85    3872.84    Other   December  2024   Other      Monday   \n",
      "209   3855.60    Other      April  2025   Other    Thursday   \n",
      "234   3725.74    Other   November  2024   Other      Friday   \n",
      "254   4067.46    Other   February  2025   Other   Wednesday   \n",
      "258   3995.20    Other   November  2023   Other    Thursday   \n",
      "263   3656.65    Other      March  2025   Other      Friday   \n",
      "265   3914.79    Other      April  2024   Other    Thursday   \n",
      "277   4257.82    Other      March  2025   Other      Friday   \n",
      "309   4296.27    Other        May  2023   Other      Sunday   \n",
      "327   3515.41    Other     August  2023   Other      Sunday   \n",
      "343   4250.26    Other      March  2025   Other    Saturday   \n",
      "448   3628.32    Other    January  2025   Other    Saturday   \n",
      "509   4286.80    Other      March  2025   Other      Friday   \n",
      "522   4170.32    Other  September  2024   Other      Monday   \n",
      "605   4013.56    Other       July  2023   Other      Friday   \n",
      "699   3938.92    Other       July  2023   Other   Wednesday   \n",
      "784   3779.16    Other   December  2024   Other   Wednesday   \n",
      "848   3959.09    Other     August  2024   Other      Monday   \n",
      "860   4490.86    Other       July  2023   Other   Wednesday   \n",
      "868   3704.12    Other    October  2023   Other      Monday   \n",
      "886   3625.35    Other   December  2023   Other      Monday   \n",
      "966   3733.24    Other      March  2025   Other      Friday   \n",
      "970   3635.89    Other      April  2024   Other   Wednesday   \n",
      "1045  4057.60    Other   February  2025   Other      Monday   \n",
      "1101  4193.52    Other       June  2024   Other      Sunday   \n",
      "1109  4255.24    Other      March  2025   Other      Sunday   \n",
      "1148  3593.85    Other      April  2025   Other      Friday   \n",
      "1165  4384.22    Other    October  2024   Other      Friday   \n",
      "1180  3981.06    Other  September  2024   Other      Monday   \n",
      "1239  4267.58    Other       July  2023   Other      Sunday   \n",
      "1278  3529.73    Other  September  2024   Other      Sunday   \n",
      "1383  4294.86    Other   December  2024   Other      Friday   \n",
      "1406  4059.06    Other   December  2023   Other      Friday   \n",
      "1563  3628.91    Other   December  2023   Other      Sunday   \n",
      "1616  4408.23    Other   February  2024   Other      Monday   \n",
      "1618  3926.30    Other      April  2025   Other      Sunday   \n",
      "1621  3929.61    Other        May  2023   Other      Sunday   \n",
      "1649  3672.90    Other      April  2024   Other    Thursday   \n",
      "1666  4126.26    Other   November  2024   Other      Friday   \n",
      "1753  3800.77    Other   December  2024   Other      Sunday   \n",
      "1815  4191.53    Other      April  2024   Other      Monday   \n",
      "1847  3584.04    Other      March  2025   Other    Thursday   \n",
      "1850  3743.27    Other    October  2024   Other    Thursday   \n",
      "1862  3573.62    Other      April  2025   Other   Wednesday   \n",
      "1979  4267.97    Other      April  2024   Other      Friday   \n",
      "1980  3536.74    Other      March  2025   Other      Friday   \n",
      "1998  4407.24    Other   December  2024   Other      Sunday   \n",
      "2027  3566.33    Other       June  2024   Other    Saturday   \n",
      "2108  4282.29    Other      March  2025   Other      Friday   \n",
      "2186  3693.09    Other      April  2025   Other    Saturday   \n",
      "\n",
      "      Is Holiday Period  Has Sale  Has Offer  Has Win  Ad Name Length  \\\n",
      "28                  0.0       0.0        0.0      1.0            78.0   \n",
      "85                  1.0       0.0        0.0      1.0            47.0   \n",
      "209                 1.0       0.0        1.0      0.0            51.0   \n",
      "234                 0.0       0.0        0.0      1.0            79.0   \n",
      "254                 0.0       0.0        0.0      1.0            52.0   \n",
      "258                 0.0       0.0        0.0      0.0            69.0   \n",
      "263                 0.0       0.0        0.0      0.0            68.0   \n",
      "265                 1.0       0.0        0.0      1.0            47.0   \n",
      "277                 0.0       0.0        0.0      1.0            50.0   \n",
      "309                 0.0       0.0        0.0      1.0            53.0   \n",
      "327                 0.0       0.0        1.0      0.0            56.0   \n",
      "343                 0.0       0.0        0.0      1.0            47.0   \n",
      "448                 0.0       0.0        0.0      0.0            68.0   \n",
      "509                 0.0       0.0        0.0      1.0            54.0   \n",
      "522                 0.0       0.0        0.0      0.0            49.0   \n",
      "605                 0.0       0.0        0.0      1.0            62.0   \n",
      "699                 0.0       0.0        0.0      1.0            78.0   \n",
      "784                 1.0       0.0        1.0      0.0            70.0   \n",
      "848                 0.0       0.0        0.0      0.0            64.0   \n",
      "860                 0.0       0.0        0.0      1.0            70.0   \n",
      "868                 0.0       0.0        0.0      1.0            80.0   \n",
      "886                 1.0       0.0        0.0      1.0            69.0   \n",
      "966                 0.0       0.0        0.0      1.0            61.0   \n",
      "970                 1.0       0.0        0.0      1.0            84.0   \n",
      "1045                0.0       0.0        0.0      1.0            74.0   \n",
      "1101                0.0       0.0        1.0      0.0            83.0   \n",
      "1109                0.0       0.0        0.0      1.0            62.0   \n",
      "1148                1.0       0.0        0.0      1.0            78.0   \n",
      "1165                0.0       0.0        0.0      0.0            84.0   \n",
      "1180                0.0       0.0        0.0      1.0            47.0   \n",
      "1239                0.0       0.0        1.0      0.0            68.0   \n",
      "1278                0.0       0.0        0.0      1.0            79.0   \n",
      "1383                1.0       0.0        0.0      1.0            83.0   \n",
      "1406                1.0       0.0        0.0      1.0            76.0   \n",
      "1563                1.0       0.0        0.0      1.0            63.0   \n",
      "1616                0.0       0.0        1.0      1.0            77.0   \n",
      "1618                1.0       0.0        0.0      1.0            50.0   \n",
      "1621                0.0       0.0        0.0      1.0            65.0   \n",
      "1649                1.0       0.0        0.0      0.0            69.0   \n",
      "1666                0.0       0.0        0.0      1.0            67.0   \n",
      "1753                1.0       0.0        0.0      1.0            83.0   \n",
      "1815                1.0       0.0        1.0      1.0            70.0   \n",
      "1847                0.0       0.0        0.0      0.0            68.0   \n",
      "1850                0.0       0.0        1.0      0.0            55.0   \n",
      "1862                1.0       0.0        1.0      1.0            81.0   \n",
      "1979                1.0       0.0        0.0      1.0            83.0   \n",
      "1980                0.0       0.0        0.0      1.0            78.0   \n",
      "1998                1.0       0.0        0.0      1.0            75.0   \n",
      "2027                0.0       0.0        1.0      1.0            77.0   \n",
      "2108                0.0       0.0        0.0      1.0            68.0   \n",
      "2186                1.0       0.0        1.0      0.0            70.0   \n",
      "\n",
      "      Actual Reach  Predicted Reach  Actual Engagement  Predicted Engagement  \n",
      "28         10418.0     26111.361328             5372.0           7397.091797  \n",
      "85         80717.0     50297.343750             5919.0            982.072266  \n",
      "209        48469.0     41895.027344             2991.0           2776.821289  \n",
      "234        24874.0     41350.054688             2720.0           1342.206055  \n",
      "254       114276.0     55029.859375               93.0           2645.907959  \n",
      "258        18700.0     41592.148438             7805.0           3233.519531  \n",
      "263        54657.0     39060.445312             2273.0           2083.988037  \n",
      "265        73004.0     49344.484375              151.0           2320.225342  \n",
      "277        54075.0     41503.902344             3451.0           2378.938965  \n",
      "309       145485.0     48767.265625             4796.0           2915.854492  \n",
      "327        82869.0     24877.408203              103.0            717.225037  \n",
      "343        64552.0     39541.210938             3049.0           1934.493408  \n",
      "448       105990.0     28609.306641             7762.0           6706.411621  \n",
      "509        53636.0     41676.164062              111.0           2409.436035  \n",
      "522        40407.0     23319.183594            10619.0          11439.551758  \n",
      "605        17570.0     22256.369141              909.0           1024.609985  \n",
      "699       100434.0     23237.396484              109.0           1834.969849  \n",
      "784        15861.0     51936.070312             1280.0            992.974243  \n",
      "848        12850.0     29834.134766              115.0           5633.332031  \n",
      "860        15994.0     24396.287109             2115.0           1939.286499  \n",
      "868        24649.0     32823.566406             4347.0           6001.378906  \n",
      "886        68151.0     32189.632812             1423.0           1446.444214  \n",
      "966        57235.0     38112.128906               89.0           2131.215332  \n",
      "970        85080.0     44242.468750              123.0           1860.995972  \n",
      "1045       19012.0     60046.691406             5379.0           2789.646973  \n",
      "1101       75010.0     57669.472656               90.0          10540.725586  \n",
      "1109       52684.0     40624.031250             3371.0           1930.685913  \n",
      "1148        7162.0     33874.960938             4289.0           1654.319702  \n",
      "1165       34550.0     26710.416016             9775.0           3117.498047  \n",
      "1180        5247.0     22407.609375             6213.0           7825.738281  \n",
      "1239       19559.0     27377.154297             2942.0           1160.054565  \n",
      "1278       18722.0     22836.898438             6057.0          10742.229492  \n",
      "1383       96684.0     57221.933594             2488.0           1546.256226  \n",
      "1406       72398.0     33105.343750              171.0           1385.771484  \n",
      "1563       17853.0     32497.072266             1489.0            858.531189  \n",
      "1616       34214.0     77859.593750            10649.0           3006.019043  \n",
      "1618       83420.0     38174.625000             5075.0           1565.259888  \n",
      "1621      166439.0     46767.042969              158.0           2956.816162  \n",
      "1649       84309.0     46153.156250              168.0           1645.312134  \n",
      "1666      111950.0     44683.511719              117.0           1618.259521  \n",
      "1753       69525.0     62353.328125             2437.0           1559.701294  \n",
      "1815       61883.0     56941.304688             2801.0           2966.744629  \n",
      "1847       20943.0     39207.496094             9751.0           2526.828613  \n",
      "1850        7968.0     26608.130859               99.0           2564.449707  \n",
      "1862       62216.0     37070.335938               37.0           2388.572510  \n",
      "1979       80389.0     47149.765625             2236.0           1939.175659  \n",
      "1980       53518.0     36448.191406             3175.0           2098.742920  \n",
      "1998       93391.0     68801.484375               42.0           1952.282959  \n",
      "2027       34518.0     46530.144531             3276.0           4996.298828  \n",
      "2108       88260.0     41223.703125             5804.0           2483.227051  \n",
      "2186       63483.0     38357.242188               67.0           1751.053345  \n",
      "\n",
      "Summary Statistics for Similar Spend Data Points:\n",
      "        Actual Reach  Actual Engagement\n",
      "count      51.000000          51.000000\n",
      "mean    56965.882353        3056.490196\n",
      "std     37084.912618        3063.634641\n",
      "min      5247.000000          37.000000\n",
      "25%     20251.000000         137.000000\n",
      "50%     54657.000000        2488.000000\n",
      "75%     81793.000000        4935.500000\n",
      "max    166439.000000       10649.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reconstruct the original test set features\n",
    "# Inverse transform the scaled features\n",
    "numerical_scaled = X_test[:, :2]  # First two columns are spend and ad_name_length\n",
    "numerical_original = feature_scaler.inverse_transform(numerical_scaled)\n",
    "spend_original = np.expm1(numerical_original[:, 0])  # Undo log for spend\n",
    "ad_name_length_original = numerical_original[:, 1]\n",
    "\n",
    "# Reconstruct binary features\n",
    "binary_features_values = X_test[:, 2:6]  # Next four columns are binary features\n",
    "\n",
    "# Reconstruct categorical features (campaign_grouped, month, year, ad_type, day_of_week)\n",
    "campaign_cols = [col for col in feature_columns if col.startswith('campaign_grouped_')]\n",
    "month_cols = [col for col in feature_columns if col.startswith('month_')]\n",
    "year_cols = [col for col in feature_columns if col.startswith('year_')]\n",
    "ad_type_cols = [col for col in feature_columns if col.startswith('ad_type_')]\n",
    "day_of_week_cols = [col for col in feature_columns if col.startswith('day_of_week_')]\n",
    "\n",
    "campaign_start_idx = 6\n",
    "month_start_idx = campaign_start_idx + len(campaign_cols)\n",
    "year_start_idx = month_start_idx + len(month_cols)\n",
    "ad_type_start_idx = year_start_idx + len(year_cols)\n",
    "day_of_week_start_idx = ad_type_start_idx + len(ad_type_cols)\n",
    "\n",
    "campaign_indices = np.argmax(X_test[:, campaign_start_idx:month_start_idx], axis=1)\n",
    "month_indices = np.argmax(X_test[:, month_start_idx:year_start_idx], axis=1)\n",
    "year_indices = np.argmax(X_test[:, year_start_idx:ad_type_start_idx], axis=1)\n",
    "ad_type_indices = np.argmax(X_test[:, ad_type_start_idx:day_of_week_start_idx], axis=1)\n",
    "day_of_week_indices = np.argmax(X_test[:, day_of_week_start_idx:], axis=1)\n",
    "\n",
    "campaign_names = [campaign_cols[idx].replace('campaign_grouped_', '') for idx in campaign_indices]\n",
    "month_names = [month_cols[idx].replace('month_', '') for idx in month_indices]\n",
    "year_names = [year_cols[idx].replace('year_', '') for idx in year_indices]\n",
    "ad_type_names = [ad_type_cols[idx].replace('ad_type_', '') for idx in ad_type_indices]\n",
    "day_of_week_names = [day_of_week_cols[idx].replace('day_of_week_', '') for idx in day_of_week_indices]\n",
    "\n",
    "# Create a DataFrame with the original test set features and actual/predicted values\n",
    "test_df_reconstructed = pd.DataFrame({\n",
    "    'Spend': spend_original,\n",
    "    'Campaign': campaign_names,\n",
    "    'Month': month_names,\n",
    "    'Year': year_names,\n",
    "    'Ad Type': ad_type_names,\n",
    "    'Day of Week': day_of_week_names,\n",
    "    'Is Holiday Period': binary_features_values[:, 0],\n",
    "    'Has Sale': binary_features_values[:, 1],\n",
    "    'Has Offer': binary_features_values[:, 2],\n",
    "    'Has Win': binary_features_values[:, 3],\n",
    "    'Ad Name Length': ad_name_length_original,\n",
    "    'Actual Reach': y_test_original[:, 0],\n",
    "    'Predicted Reach': y_pred_original[:, 0],\n",
    "    'Actual Engagement': y_test_original[:, 1],\n",
    "    'Predicted Engagement': y_pred_original[:, 1]\n",
    "})\n",
    "\n",
    "# Step 6 prediction for comparison\n",
    "step6_pred_reach = predictions[0, 0]  # Use the prediction from Cell 6\n",
    "step6_pred_engagement = predictions[0, 1]\n",
    "step6_spend = 4000\n",
    "step6_campaign = campaign_name\n",
    "\n",
    "# Find data points with the same campaign\n",
    "same_campaign_df = test_df_reconstructed[\n",
    "    test_df_reconstructed['Campaign'] == step6_campaign\n",
    "]\n",
    "\n",
    "# Find data points with similar spend (within ±500 of 4000)\n",
    "similar_spend_df = test_df_reconstructed[\n",
    "    (test_df_reconstructed['Spend'] >= 3500) & (test_df_reconstructed['Spend'] <= 4500)\n",
    "]\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nStep 6 Prediction (Spend={step6_spend}, Campaign={step6_campaign}):\")\n",
    "print(f\"Predicted Reach: {step6_pred_reach:.2f}, Predicted Engagement: {step6_pred_engagement:.2f}\")\n",
    "\n",
    "print(\"\\nData Points with the Same Campaign:\")\n",
    "if not same_campaign_df.empty:\n",
    "    print(same_campaign_df)\n",
    "else:\n",
    "    print(\"No data points found with the same campaign in the test set.\")\n",
    "\n",
    "print(\"\\nData Points with Similar Spend (3500 ≤ Spend ≤ 4500):\")\n",
    "if not similar_spend_df.empty:\n",
    "    print(similar_spend_df)\n",
    "    print(\"\\nSummary Statistics for Similar Spend Data Points:\")\n",
    "    print(similar_spend_df[['Actual Reach', 'Actual Engagement']].describe())\n",
    "else:\n",
    "    print(\"No data points found with similar spend in the test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
