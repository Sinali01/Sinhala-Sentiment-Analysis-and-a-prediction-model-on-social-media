{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Emoji Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining emoji mappings\n",
    "emoji_to_description = {\n",
    "    \"üòÄ\": \"grin\",\n",
    "    \"üòä\": \"smile\",\n",
    "    \"üòÇ\": \"tear of joy\",\n",
    "    \"ü§£\": \"tear of joy\",\n",
    "    \"üòá\": \"halo\",\n",
    "    \"üòâ\": \"wink\",\n",
    "    \"üòã\": \"savor food\",\n",
    "    \"üòé\": \"sunglass\",\n",
    "    \"‚òÄÔ∏è\": \"sun\",\n",
    "    \"üòò\": \"blow a kiss\",\n",
    "    \"üò∫\": \"cat\",\n",
    "    \"‚ù§Ô∏è\": \"red heart\",\n",
    "    \"üíï\": \"two heart\",\n",
    "    \"üò°\": \"angri\",\n",
    "    \"üòê\": \"neutral\",\n",
    "    \"üòë\": \"expressionless\",\n",
    "    \"üòï\": \"confound\",\n",
    "    \"üò§\": \"steam from nose\",\n",
    "    \"üí©\": \"pile of poo\",\n",
    "    \"üòì\": \"sweat\",\n",
    "    \"‚òπÔ∏è\": \"frown\",\n",
    "    \"üò±\": \"scream fear\",\n",
    "    \"üò∞\": \"anxious\",\n",
    "    \"üòî\": \"pensiv\",\n",
    "    \"üò©\": \"tired\",\n",
    "    \"üò¢\": \"cri\",\n",
    "    \"üò•\": \"sad\",\n",
    "    \"üò¥\": \"sleepi\",\n",
    "    \"üòû\": \"downcast\",\n",
    "    \"üíî\": \"broken heart\",\n",
    "    \"üò≥\": \"flush\",\n",
    "    \"ü§´\": \"hush\",\n",
    "    \"üòµ\": \"knockedout\",\n",
    "    \"üò≤\": \"astonish\"\n",
    "}\n",
    "\n",
    "emoji_meanings = {\n",
    "    \"grin\": \"‡∑É‡∑í‡∂±‡∑Ñ‡∑Ä\",\n",
    "    \"face\": \"‡∂∏‡∑î‡∑Ñ‡∑î‡∂´\",\n",
    "    \"with\": \"‡∑É‡∂∏‡∂ú\",\n",
    "    \"tear\": \"‡∂ö‡∂≥‡∑î‡∑Ö‡∑î\",\n",
    "    \"of\": \"‡∑Ä‡∂Ω\",\n",
    "    \"joy\": \"‡∑É‡∂≠‡∑î‡∂ß\",\n",
    "    \"big\": \"‡∂∏‡∑Ñ‡∑è\",\n",
    "    \"eye\": \"‡∂á‡∑É\",\n",
    "    \"smile\": \"‡∑É‡∑í‡∂±‡∑Ñ‡∑Ä\",\n",
    "    \"squint\": \"‡∂á‡∑É‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑í‡∑É‡∑ô‡∂±‡∑Ä‡∑è\",\n",
    "    \"halo\": \"‡∑Ñ‡∂Ω‡∑ù\",\n",
    "    \"wink\": \"‡∂á‡∑É‡∑í‡∂¥‡∑í‡∂∫ ‡∑Ñ‡∑ô‡∑Ö‡∂±‡∑ä‡∂±\",\n",
    "    \"savor\": \"‡∂ª‡∑É ‡∑Ä‡∑í‡∂≥‡∑í‡∂±‡∑ä‡∂±\",\n",
    "    \"food\": \"‡∂Ü‡∑Ñ‡∑è‡∂ª\",\n",
    "    \"reliev\": \"‡∑É‡∑Ñ‡∂±‡∂∫\",\n",
    "    \"heartey\": \"‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑è‡∂Ç‡∂ú‡∂∏\",\n",
    "    \"sunglass\": \"‡∑Ñ‡∑í‡∂ª‡∑î ‡∑Ä‡∑ì‡∂Ø‡∑î‡∂ª‡∑î\",\n",
    "    \"sun\": \"‡∑Ñ‡∑í‡∂ª‡∑î\",\n",
    "    \"blow\": \"‡∂¥‡∑í‡∂π‡∑ì‡∂∏\",\n",
    "    \"a\": \"‡∂í\",\n",
    "    \"kiss\": \"‡∑Ñ‡∑è‡∂Ø‡∑î‡∑Ä‡∂ö‡∑ä\",\n",
    "    \"tongue\": \"‡∂Ø‡∑í‡∑Ä\",\n",
    "    \"cat\": \"‡∂∂‡∑Ö‡∂Ω‡∑è\",\n",
    "    \"red\": \"‡∂ª‡∂≠‡∑î\",\n",
    "    \"heart\": \"‡∑Ñ‡∂Ø‡∑Ä‡∂≠\",\n",
    "    \"two\": \"‡∂Ø‡∑ô‡∂ö\",\n",
    "    \"beam\": \"‡∂ö‡∂Ø‡∂∏‡∑ä‡∂∂\",\n",
    "    \"suit\": \"‡∂á‡∂≥‡∑î‡∂∏\",\n",
    "    \"grimac\": \"\",\n",
    "    \"angri\": \"‡∂ö‡∑ù‡∂¥‡∂∫‡∑ô‡∂±‡∑ä\",\n",
    "    \"neutral\": \"‡∂∏‡∂∞‡∑ä‡∂∫‡∑É‡∑ä‡∂Æ\",\n",
    "    \"expressionless\": \"‡∂¥‡∑ä‡∂ª‡∂ö‡∑è‡∑Å ‡∂ª‡∑Ñ‡∑í‡∂≠\",\n",
    "    \"pout\": \"‡∂¥‡∑î‡∂ß‡∑ä\",\n",
    "    \"confound\": \"‡∑Ä‡∑ä‡∂∫‡∑è‡∂ö‡∑ñ‡∂Ω ‡∂ö‡∂ª‡∂∫‡∑í\",\n",
    "    \"steam\": \"‡∑Ä‡∑è‡∑Ç‡∑ä‡∂¥\",\n",
    "    \"from\": \"‡∑É‡∑í‡∂ß\",\n",
    "    \"nose\": \"‡∂±‡∑è‡∑É‡∂∫\",\n",
    "    \"pile\": \"‡∂ú‡∑ú‡∂©‡∑Ä‡∂Ω‡∑ä\",\n",
    "    \"poo\": \"‡∂†‡∑ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±\",\n",
    "    \"sweat\": \"‡∂Ø‡∑Ñ‡∂©‡∑í‡∂∫\",\n",
    "    \"frown\": \"‡∂±‡∑Ö‡∂Ω ‡∂ª‡∑ê‡∂Ω‡∑í ‡∂ú‡∂±‡∑í‡∂∫‡∑í\",\n",
    "    \"mouth\": \"‡∂∏‡∑î‡∂õ‡∂∫\",\n",
    "    \"scream\": \"‡∂ö‡∑ë‡∂ú‡∑Ñ‡∂±‡∑Ä‡∑è\",\n",
    "    \"fear\": \"‡∂∂‡∑í‡∂∫\",\n",
    "    \"anxious\": \"‡∂ö‡∂±‡∑É‡∑ä‡∑É‡∂Ω‡∑ä‡∂Ω‡∑ô‡∂±‡∑ä\",\n",
    "    \"pensiv\": \"‡∂ö‡∂Ω‡∑ä‡∂¥‡∂±‡∑è‡∂ö‡∑è‡∂ª‡∑ì\",\n",
    "    \"weari\": \"‡∂Ö‡∂≥‡∑í‡∂±‡∑Ä‡∑è\",\n",
    "    \"confus\": \"‡∑Ä‡∑ä‡∂∫‡∑è‡∂ö‡∑ñ‡∂Ω ‡∂ö‡∂ª‡∂∫‡∑í\",\n",
    "    \"tired\": \"‡∂∏‡∑Ñ‡∂±‡∑ä‡∑É‡∑í‡∂∫‡∑í\",\n",
    "    \"cri\": \"‡∂Ö‡∂¨‡∂±‡∑Ä‡∑è\",\n",
    "    \"sad\": \"‡∂Ø‡∑î‡∂ö\",\n",
    "    \"sleepi\": \"‡∂±‡∑í‡∂Ø‡∑è‡∂ú‡∂±‡∑ä‡∂±\",\n",
    "    \"downcast\": \"‡∂¥‡∑Ñ‡∂≠‡∂ß ‡∑Ä‡∑ê‡∂ß‡∑ì ‡∂á‡∂≠\",\n",
    "    \"loudli\": \"‡∑Ñ‡∂∫‡∑í‡∂∫‡∑ô‡∂±‡∑ä\",\n",
    "    \"broken\": \"‡∂ö‡∑ê‡∂©‡∑î‡∂´‡∑î\",\n",
    "    \"flush\": \"‡∑Ü‡∑ä‡∂Ω‡∑Ç‡∑ä\",\n",
    "    \"hush\": \"‡∂±‡∑í‡∑Ñ‡∂¨‡∂∫‡∑í\",\n",
    "    \"knockedout\": \"‡∂≠‡∂ß‡∑ä‡∂ß‡∑î ‡∂ö‡∑Ö‡∑è\",\n",
    "    \"astonish\": \"‡∑Ä‡∑í‡∑É‡∑ä‡∂∏‡∑í‡∂≠‡∂∫‡∑í\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_url</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>video_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.facebook.com/sinhala.adaderana.lk/...</td>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.facebook.com/sinhala.adaderana.lk/...</td>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.facebook.com/sinhala.adaderana.lk/...</td>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.facebook.com/sinhala.adaderana.lk/...</td>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.facebook.com/sinhala.adaderana.lk/...</td>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            post_url  \\\n",
       "0  https://www.facebook.com/sinhala.adaderana.lk/...   \n",
       "1  https://www.facebook.com/sinhala.adaderana.lk/...   \n",
       "2  https://www.facebook.com/sinhala.adaderana.lk/...   \n",
       "3  https://www.facebook.com/sinhala.adaderana.lk/...   \n",
       "4  https://www.facebook.com/sinhala.adaderana.lk/...   \n",
       "\n",
       "                                              author  \\\n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...   \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±   \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...   \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...   \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫   \n",
       "\n",
       "                                                text video_url  \n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...       NaN  \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±       NaN  \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...       NaN  \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...       NaN  \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading and combine the datasets\n",
    "\n",
    "fb_data = pd.read_csv('Data/facebook_comments.csv')\n",
    "yt_data = pd.read_csv('Data/channel_comments.csv')\n",
    "combined_data = pd.concat([fb_data, yt_data], ignore_index=True)\n",
    "\n",
    "# checking for NaN values in the 'text' column and replacing them with empty strings\n",
    "combined_data['text'] = combined_data['text'].fillna('')\n",
    "\n",
    "# ensuring the 'text' column is of string type\n",
    "combined_data['text'] = combined_data['text'].astype(str)\n",
    "\n",
    "# displaying the first few rows to verify\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...   \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±   \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...   \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...   \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫   \n",
       "\n",
       "                                      processed_text  \n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...  \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±  \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...  \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...  \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing emojis by converting them to Sinhala descriptions\n",
    "def preprocess_emojis(text):\n",
    "    # ensuring text is a string and handle empty/None values\n",
    "    text = str(text) if text else ''\n",
    "    \n",
    "    # skipping processing if text is empty\n",
    "    if not text.strip():\n",
    "        return text\n",
    "    \n",
    "    # replacing emojis with their Sinhala descriptions\n",
    "    for emoji, desc in emoji_to_description.items():\n",
    "        if emoji in text:\n",
    "            # splitting description into words and translate each\n",
    "            desc_words = desc.split()\n",
    "            sinhala_desc = ' '.join(emoji_meanings.get(word, word) for word in desc_words)\n",
    "            text = text.replace(emoji, f' {sinhala_desc} ')\n",
    "    return text\n",
    "\n",
    "# applying preprocessing to the 'text' column\n",
    "combined_data['processed_text'] = combined_data['text'].apply(preprocess_emojis)\n",
    "\n",
    "# displaying a sample to verify\n",
    "combined_data[['text', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export a Subset of Comments for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 500 comments to 'to_label.csv'. Please manually add a 'label' column with values 'positive', 'negative', or 'neutral', then save the file as 'labeled_comments.csv'.\n"
     ]
    }
   ],
   "source": [
    "# exporting a subset of comments for manual labeling\n",
    "# selecting 500 comments (adjust the number as needed)\n",
    "sample_to_label = combined_data[['text']].head(500)\n",
    "\n",
    "# saving to a CSV file for manual labeling\n",
    "sample_to_label.to_csv('to_label.csv', index=False)\n",
    "\n",
    "print(\"Exported 500 comments to 'to_label.csv'. Please manually add a 'label' column with values 'positive', 'negative', or 'neutral', then save the file as 'labeled_comments.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect and Clean the Labeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting 'labeled_comments.csv':\n",
      "Total rows: 195\n",
      "\n",
      "Missing values in 'label' column: 1\n",
      "\n",
      "Unique values in 'label' column:\n",
      "label\n",
      "negative    131\n",
      "neutral      36\n",
      "positive     27\n",
      "NaN           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rows after cleaning: 194\n",
      "Rows removed: 1\n",
      "\n",
      "Cleaned dataset saved to 'labeled_comments_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# loading the labeled dataset as a DataFrame to inspect and clean\n",
    "labeled_data = pd.read_csv('Data/labeled_comments.csv')\n",
    "\n",
    "# checking for missing or invalid labels\n",
    "print(\"Inspecting 'labeled_comments.csv':\")\n",
    "print(\"Total rows:\", len(labeled_data))\n",
    "print(\"\\nMissing values in 'label' column:\", labeled_data['label'].isna().sum())\n",
    "print(\"\\nUnique values in 'label' column:\")\n",
    "print(labeled_data['label'].value_counts(dropna=False))\n",
    "\n",
    "# cleaning the dataset\n",
    "# removing rows where 'label' is NaN or not in ['positive', 'negative', 'neutral']\n",
    "valid_labels = ['positive', 'negative', 'neutral']\n",
    "labeled_data_cleaned = labeled_data[labeled_data['label'].isin(valid_labels) & labeled_data['label'].notna()]\n",
    "\n",
    "# checking how many rows were removed\n",
    "print(\"\\nRows after cleaning:\", len(labeled_data_cleaned))\n",
    "print(\"Rows removed:\", len(labeled_data) - len(labeled_data_cleaned))\n",
    "\n",
    "# saving the cleaned dataset\n",
    "labeled_data_cleaned.to_csv('labeled_comments_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved to 'labeled_comments_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune the XLM-RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764a8dd4ead4de99b32406dab4ed0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Step 10/20, Loss: 1.0326\n",
      "Step 20/20, Loss: 0.8523\n",
      "Average training loss: 1.0085\n",
      "Average evaluation loss: 1.0083\n",
      "Epoch 2/3\n",
      "Step 10/20, Loss: 0.4134\n",
      "Step 20/20, Loss: 0.6536\n",
      "Average training loss: 0.8317\n",
      "Average evaluation loss: 0.9612\n",
      "Epoch 3/3\n",
      "Step 10/20, Loss: 1.1427\n",
      "Step 20/20, Loss: 0.4896\n",
      "Average training loss: 0.8548\n",
      "Average evaluation loss: 1.0763\n",
      "Fine-tuning complete. Model saved to 'fine_tuned_xlm_roberta'.\n",
      "Model is on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# importing torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW  # Updated import for AdamW\n",
    "from transformers import get_linear_schedule_with_warmup  \n",
    "\n",
    "# loading the cleaned labeled dataset for fine-tuning\n",
    "dataset = load_dataset('csv', data_files='labeled_comments_cleaned.csv')\n",
    "\n",
    "# mapping labels to integers\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "dataset = dataset.map(lambda x: {'label': label_map[x['label']]})\n",
    "\n",
    "# loading the XLM-RoBERTa model and tokenizer\n",
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# tokenizing the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# preparing the dataset for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# splitting into train and validation sets (80% train, 20% validation)\n",
    "train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(int(0.8 * len(tokenized_datasets['train']))))\n",
    "eval_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(int(0.8 * len(tokenized_datasets['train'])), len(tokenized_datasets['train'])))\n",
    "\n",
    "# creating DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# moving model to GPU if available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# defining optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs  # ~60 steps (155 examples / batch_size 8 * 3 epochs)\n",
    "warmup_steps = 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # moving batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # log training loss every 10 steps\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"Step {step + 1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    print(f\"Average evaluation loss: {avg_eval_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# saving the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_xlm_roberta')\n",
    "tokenizer.save_pretrained('fine_tuned_xlm_roberta')\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved to 'fine_tuned_xlm_roberta'.\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions for Tokenization and Sentiment Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and encode the text\n",
    "def encode_text(text, max_length=128):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # moving inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    inputs = encode_text(text)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()[0]  # moving to CPU for numpy conversion\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    sentiment = labels[np.argmax(probs)]\n",
    "    return sentiment, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "      <td>‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.74273384, 0.13335575, 0.12391042]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "      <td>‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.85549426, 0.08701834, 0.057487454]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "      <td>‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.7033787, 0.16769668, 0.12892465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "      <td>‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.5697927, 0.2342302, 0.19597708]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "      <td>‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.6523738, 0.20113891, 0.14648731]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...   \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±   \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...   \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...   \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫   \n",
       "\n",
       "                                      processed_text sentiment  \\\n",
       "0  ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\\n‡∂Ö‡∑Ñ‡∑É...  negative   \n",
       "1     ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±  negative   \n",
       "2  ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\\n‚Äç\\n‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä...  negative   \n",
       "3  ‡∂∂‡∑ú‡∂ª‡∑î ‡∂ú‡∂±‡∂±‡∑ä ‡∂Ø‡∑í‡∂Ω‡∑è ‡∂Ö‡∂≠‡∑ä\\n‚Äç\\n‡∂∫‡∑Ä‡∑Å‡∑ä\\n‚Äç\\n‡∂∫ ‡∂∏ ‡∂±‡∑ú‡∑Ä‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ú...  negative   \n",
       "4                                           ‡∂¥‡∑î‡∂¥‡∑î‡∂ª‡∑ê‡∂Ø‡∂∫  negative   \n",
       "\n",
       "                 sentiment_probabilities  \n",
       "0   [0.74273384, 0.13335575, 0.12391042]  \n",
       "1  [0.85549426, 0.08701834, 0.057487454]  \n",
       "2    [0.7033787, 0.16769668, 0.12892465]  \n",
       "3     [0.5697927, 0.2342302, 0.19597708]  \n",
       "4    [0.6523738, 0.20113891, 0.14648731]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying sentiment analysis to all comments\n",
    "sentiments = []\n",
    "probabilities = []\n",
    "\n",
    "for text in combined_data['processed_text']:\n",
    "    sentiment, probs = predict_sentiment(text)\n",
    "    sentiments.append(sentiment)\n",
    "    probabilities.append(probs)\n",
    "\n",
    "# adding results to the DataFrame\n",
    "combined_data['sentiment'] = sentiments\n",
    "combined_data['sentiment_probabilities'] = probabilities\n",
    "\n",
    "# displaying a sample of the results\n",
    "combined_data[['text', 'processed_text', 'sentiment', 'sentiment_probabilities']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Sentiment Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments per Sentiment:\n",
      "sentiment\n",
      "negative    620\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage Distribution of Sentiments:\n",
      "sentiment\n",
      "negative    100.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Average Confidence (Probability) per Sentiment:\n",
      "sentiment\n",
      "negative    0.654124\n",
      "Name: predicted_prob, dtype: float32\n",
      "\n",
      "Sample Comments for Each Sentiment:\n",
      "\n",
      "Sentiment: positive\n",
      "\n",
      "Sentiment: negative\n",
      "- Text: ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\n",
      "‡∂Ö‡∑Ñ‡∑É ‡∂ã‡∑É‡∂ß ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∑Ä‡∂Ω‡∂ß ‡∂∂‡∂Ø‡∑î ‡∂ú‡∑Ñ‡∂Ω ‡∑Ä‡∑è‡∑Ñ‡∂±‡∂∫‡∂ö‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∑ê‡∂ª‡∑í ‡∂≠‡∂ª‡∂∏‡∂ß ‡∂í‡∂ö ‡∑Ñ‡∑ì‡∂±‡∂∫‡∂ö‡∑ä ‡∂ö‡∂ª‡∂Ω\n",
      "‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∂‡∂Ø‡∑î ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂Ü‡∂Ø‡∑è‡∂∫‡∂∏‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∂Ω‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂± ‡∂∏‡∑î‡∂±‡∑ä‡∑Ä ‡∂±‡∂∏‡∑ä ‡∂∏‡∑í‡∂±‡∑í‡∑É‡∑ä‡∑É‡∑î ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂∂‡∑ë\n",
      "  Processed Text: ‡∂∏‡∑ö ‡∑Ä‡∂ú‡∑ô ‡∂∂‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä‡∂Ø ‡∂∂‡∂±‡∑ä ‡∂Ö‡∂±‡∑î‡∂ª‡∂∫‡∂ú‡∑ô ‡∂Ü‡∂ª‡∑ä‡∂≠‡∑í‡∂ö ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö?\n",
      "‡∂Ö‡∑Ñ‡∑É ‡∂ã‡∑É‡∂ß ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∑Ä‡∂Ω‡∂ß ‡∂∂‡∂Ø‡∑î ‡∂ú‡∑Ñ‡∂Ω ‡∑Ä‡∑è‡∑Ñ‡∂±‡∂∫‡∂ö‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∑ê‡∂ª‡∑í ‡∂≠‡∂ª‡∂∏‡∂ß ‡∂í‡∂ö ‡∑Ñ‡∑ì‡∂±‡∂∫‡∂ö‡∑ä ‡∂ö‡∂ª‡∂Ω\n",
      "‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∂‡∂Ø‡∑î ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂Ü‡∂Ø‡∑è‡∂∫‡∂∏‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∂Ω‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂± ‡∂∏‡∑î‡∂±‡∑ä‡∑Ä ‡∂±‡∂∏‡∑ä ‡∂∏‡∑í‡∂±‡∑í‡∑É‡∑ä‡∑É‡∑î ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂∂‡∑ë\n",
      "  Confidence: 0.7427\n",
      "- Text: ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±\n",
      "  Processed Text: ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø ‡∂±‡∂∏‡∂∫‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∂±‡∑Ä‡∑è ‡∑Ä‡∑í‡∂ß‡∑ä‡∑É‡∑ä ‡∂ë‡∂ö ‡∂Ω‡∂ö‡∑ä‡∑Ç 12 ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±\n",
      "  Confidence: 0.8555\n",
      "- Text: ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\n",
      "‚Äç\n",
      "‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä‡∂ß. ‡∂∏‡∑ú‡∂ö‡∂Ø ‡∂ã‡∂±‡∑ä ‡∂Ø‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∑Ä‡∑è‡∑Ñ‡∂±‡∂∫‡∂ö‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂≠‡∑í‡∂∫‡∑è ‡∂ë‡∂Ø‡∑è‡∑Ä‡∑ö‡∂Ω ‡∂ö‡∂±‡∑ä‡∂± ‡∂∂‡∑ê‡∂ª‡∑í ‡∂≠‡∂ª‡∂∏‡∑ä. ‡∂ö‡∑ê‡∂Ω‡∂´‡∑í ‡∑Ñ‡∑í‡∂ü‡∂±‡∑ä‡∂±‡∑ù ‡∂≠‡∂∏‡∂∫‡∑í ‡∂ã‡∂±‡∑ä‡∂ß ‡∂†‡∂±‡∑ä‡∂Ø‡∑ô ‡∂Ø‡∑î‡∂±‡∑ä‡∂±‡∑ô ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è.\n",
      "  Processed Text: ‡∑Ä‡∑è‡∑Ñ‡∂± ‡∂∏‡∑í‡∂Ω ‡∂∏‡∑ú‡∂±‡∑Ä‡∑è ‡∑Ä‡∑î‡∂±‡∂≠‡∑ä ‡∂¥‡∑ä\n",
      "‚Äç\n",
      "‡∂ª‡∑Å‡∑ä‡∂±‡∂∫‡∂ö‡∑ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ã‡∂±‡∑ä‡∂ß. ‡∂∏‡∑ú‡∂ö‡∂Ø ‡∂ã‡∂±‡∑ä ‡∂Ø‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∑Ä‡∑è‡∑Ñ‡∂±‡∂∫‡∂ö‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂≠‡∑í‡∂∫‡∑è ‡∂ë‡∂Ø‡∑è‡∑Ä‡∑ö‡∂Ω ‡∂ö‡∂±‡∑ä‡∂± ‡∂∂‡∑ê‡∂ª‡∑í ‡∂≠‡∂ª‡∂∏‡∑ä. ‡∂ö‡∑ê‡∂Ω‡∂´‡∑í ‡∑Ñ‡∑í‡∂ü‡∂±‡∑ä‡∂±‡∑ù ‡∂≠‡∂∏‡∂∫‡∑í ‡∂ã‡∂±‡∑ä‡∂ß ‡∂†‡∂±‡∑ä‡∂Ø‡∑ô ‡∂Ø‡∑î‡∂±‡∑ä‡∂±‡∑ô ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è.\n",
      "  Confidence: 0.7034\n",
      "\n",
      "Sentiment: neutral\n",
      "\n",
      "Evaluation summary saved to 'sentiment_evaluation_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# evaluating sentiment analysis results\n",
    "\n",
    "# counting the number of comments for each sentiment\n",
    "sentiment_counts = combined_data['sentiment'].value_counts()\n",
    "print(\"Number of Comments per Sentiment:\")\n",
    "print(sentiment_counts)\n",
    "print()\n",
    "\n",
    "# calculating the percentage distribution of sentiments\n",
    "sentiment_percentages = combined_data['sentiment'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage Distribution of Sentiments:\")\n",
    "print(sentiment_percentages)\n",
    "print()\n",
    "\n",
    "# calculating the average probability (confidence) for each sentiment\n",
    "# extracting the probability corresponding to the predicted sentiment\n",
    "def get_predicted_prob(row):\n",
    "    sentiment_idx = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    idx = sentiment_idx[row['sentiment']]\n",
    "    return row['sentiment_probabilities'][idx]\n",
    "\n",
    "combined_data['predicted_prob'] = combined_data.apply(get_predicted_prob, axis=1)\n",
    "avg_probabilities = combined_data.groupby('sentiment')['predicted_prob'].mean()\n",
    "print(\"Average Confidence (Probability) per Sentiment:\")\n",
    "print(avg_probabilities)\n",
    "print()\n",
    "\n",
    "# displaying sample comments for each sentiment (up to 3 examples per sentiment)\n",
    "print(\"Sample Comments for Each Sentiment:\")\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    print(f\"\\nSentiment: {sentiment}\")\n",
    "    # filtering comments for the current sentiment\n",
    "    sample_comments = combined_data[combined_data['sentiment'] == sentiment][['text', 'processed_text', 'predicted_prob']].head(3)\n",
    "    for idx, row in sample_comments.iterrows():\n",
    "        print(f\"- Text: {row['text']}\")\n",
    "        print(f\"  Processed Text: {row['processed_text']}\")\n",
    "        print(f\"  Confidence: {row['predicted_prob']:.4f}\")\n",
    "\n",
    "# saving the evaluation summary to a CSV file\n",
    "# creating a summary DataFrame\n",
    "summary_data = {\n",
    "    'Sentiment': sentiment_counts.index,\n",
    "    'Count': sentiment_counts.values,\n",
    "    'Percentage': sentiment_percentages.values,\n",
    "    'Average_Confidence': avg_probabilities.values\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv('sentiment_evaluation_summary.csv', index=False)\n",
    "print(\"\\nEvaluation summary saved to 'sentiment_evaluation_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in labeled_data: Index(['text', 'label'], dtype='object')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      1.00      0.81       131\n",
      "     neutral       0.00      0.00      0.00        36\n",
      "    positive       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.68       194\n",
      "   macro avg       0.23      0.33      0.27       194\n",
      "weighted avg       0.46      0.68      0.54       194\n",
      "\n",
      "Classification report saved as 'classification_report.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n",
    "\n",
    "# loading the labeled dataset directly\n",
    "labeled_data = pd.read_csv('labeled_comments_cleaned.csv')\n",
    "\n",
    "# checking the columns in labeled_data (for debugging)\n",
    "print(\"Columns in labeled_data:\", labeled_data.columns)\n",
    "\n",
    "# mapping the sentiment labels back to integers for comparison\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "reverse_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# getting the true labels from the labeled dataset\n",
    "true_labels = labeled_data['label'].map(label_map).values\n",
    "\n",
    "# applying predictions to the labeled dataset\n",
    "sentiments = []\n",
    "for text in labeled_data['text']:\n",
    "    sentiment, _ = predict_sentiment(text)\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "# mapping predicted sentiments to integers\n",
    "predicted_labels = pd.Series(sentiments).map(label_map).values\n",
    "\n",
    "# generating the classification report\n",
    "\n",
    "# including all classes the model was trained on: negative (0), neutral (1), positive (2)\n",
    "report = classification_report(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    target_names=['negative', 'neutral', 'positive'],\n",
    "    output_dict=False,\n",
    "    zero_division=0  # Suppress warnings\n",
    ")\n",
    "\n",
    "# printing the classification report\n",
    "print(report)\n",
    "\n",
    "# converting the classification report to a dictionary for saving to CSV\n",
    "report_dict = classification_report(\n",
    "    true_labels,\n",
    "    predicted_labels,\n",
    "    target_names=['negative', 'neutral', 'positive'],\n",
    "    output_dict=True,\n",
    "    zero_division=0  # Suppress warnings\n",
    ")\n",
    "\n",
    "# preparing data for CSV\n",
    "report_data = []\n",
    "# adding per-class metrics\n",
    "for label in ['negative', 'neutral', 'positive']:\n",
    "    metrics = report_dict[label]\n",
    "    report_data.append({\n",
    "        '': label,\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1-score': metrics['f1-score'],\n",
    "        'support': metrics['support']\n",
    "    })\n",
    "\n",
    "# adding accuracy, macro avg, and weighted avg\n",
    "report_data.append({\n",
    "    '': 'accuracy',\n",
    "    'precision': '',\n",
    "    'recall': '',\n",
    "    'f1-score': report_dict['accuracy'],\n",
    "    'support': report_dict['weighted avg']['support']\n",
    "})\n",
    "report_data.append({\n",
    "    '': 'macro avg',\n",
    "    'precision': report_dict['macro avg']['precision'],\n",
    "    'recall': report_dict['macro avg']['recall'],\n",
    "    'f1-score': report_dict['macro avg']['f1-score'],\n",
    "    'support': report_dict['macro avg']['support']\n",
    "})\n",
    "report_data.append({\n",
    "    '': 'weighted avg',\n",
    "    'precision': report_dict['weighted avg']['precision'],\n",
    "    'recall': report_dict['weighted avg']['recall'],\n",
    "    'f1-score': report_dict['weighted avg']['f1-score'],\n",
    "    'support': report_dict['weighted avg']['support']\n",
    "})\n",
    "\n",
    "# saving the classification report to a CSV file\n",
    "report_df = pd.DataFrame(report_data)\n",
    "report_df.to_csv('classification_report.csv', index=False)\n",
    "print(\"Classification report saved as 'classification_report.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
